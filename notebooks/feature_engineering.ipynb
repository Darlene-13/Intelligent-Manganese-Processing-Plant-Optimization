{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Manganese Processing Plant Feature Engineering\n",
    "#### Advanced feature creation for ML optimization models\n",
    "\n",
    "#### AUTHOR: DARLENE WENDY NASIMIYU\n",
    "#### Purpose: Create powerful features for manganese processing optimization"
   ],
   "id": "870d51ca7d847d5c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T04:49:50.264504Z",
     "start_time": "2025-10-08T04:49:50.254860Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os"
   ],
   "id": "6102ea71b04c1724",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T04:49:50.336180Z",
     "start_time": "2025-10-08T04:49:50.327534Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#----SETUP: Get absolute path ---\n",
    "BASE_DIR = os.path.dirname(os.getcwd()) # Current working directory of the notebook\n",
    "data_dir = os.path.join(BASE_DIR, 'data', 'synthetic')\n",
    "\n",
    "print(\"Using data directory:\", data_dir)"
   ],
   "id": "41d94e6bd90c4e93",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data directory: /home/darlenewendie/PycharmProjects/Intelligent-Manganese-Processing-Plant-Optimization/data/synthetic\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T04:49:51.229605Z",
     "start_time": "2025-10-08T04:49:50.484609Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -----DEFINE DATASET FILES-------\n",
    "dataset_files = {\n",
    "    'ore_feed': 'manganese_ore_feed.csv',\n",
    "    'blended_ore': 'manganese_blended_ore_feed.csv',\n",
    "    'crushing': 'manganese_crushing_circuit.csv',\n",
    "    'separation': 'manganese_separation_circuit.csv',\n",
    "    'flotation': 'manganese_flotation_circuit.csv',\n",
    "    'dms': 'manganese_dms_circuit.csv',\n",
    "    'jigging':'manganese_jigging_circuit.csv',\n",
    "    'dewatering': 'manganese_dewatering_circuit.csv',\n",
    "    'equipment': 'manganese_equipment_health.csv',\n",
    "    'energy': 'manganese_energy_consumption.csv',\n",
    "}\n",
    "# Load all datasets\n",
    "datasets = {}\n",
    "for name, filename in dataset_files.items():\n",
    "    filepath  = os.path.join(data_dir, filename)\n",
    "    try:\n",
    "        df = pd.read_csv(filepath, parse_dates=['timestamp'])\n",
    "        datasets[name] = df\n",
    "        print(f\" Loaded {name}: {len(df):,} records, {len(df.columns)} columns\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Could not find {filepath}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {name}: {str(e)}\")\n",
    "\n",
    "print(f\"\\nTotal datasets loaded: {len(datasets)}\")\n",
    "print(f\"Total records: {sum(len(df) for df in datasets.values()):,}\")"
   ],
   "id": "5ee83316890667d1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded ore_feed: 10,000 records, 11 columns\n",
      " Loaded blended_ore: 6,522 records, 11 columns\n",
      " Loaded crushing: 15,000 records, 9 columns\n",
      " Loaded separation: 12,000 records, 13 columns\n",
      " Loaded flotation: 12,000 records, 22 columns\n",
      " Loaded dms: 8,000 records, 16 columns\n",
      " Loaded jigging: 10,000 records, 16 columns\n",
      " Loaded dewatering: 8,000 records, 18 columns\n",
      " Loaded equipment: 8,000 records, 12 columns\n",
      " Loaded energy: 10,000 records, 30 columns\n",
      "\n",
      "Total datasets loaded: 10\n",
      "Total records: 99,522\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T04:49:51.480486Z",
     "start_time": "2025-10-08T04:49:51.471675Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#INITIALIZING AN EMPTY DICTIONARY FOR ENGINEERED DATASETS\n",
    "engineered_datasets = {}\n"
   ],
   "id": "5c4d7cc0f459d817",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T04:49:51.631640Z",
     "start_time": "2025-10-08T04:49:51.544808Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# CATEGORY 1: ORE CHARACTERISTICS FEATURES\n",
    "def engineer_ore_features(ore_df):\n",
    "    \"\"\"Generate ore characteristics features (Features 1-18)\"\"\"\n",
    "    print(\"\\nEngineering Ore Characteristics Features...\")\n",
    "\n",
    "    ore_data = ore_df.copy()\n",
    "\n",
    "    # Basic transformations (1-7)\n",
    "    ore_data['mn_grade_squared'] = ore_data['mn_grade_pct'] ** 2\n",
    "    ore_data['mn_grade_log'] = np.log1p(ore_data['mn_grade_pct'])\n",
    "\n",
    "    ore_data['gangue_total'] = (ore_data['fe_content_pct'] + ore_data['siO2_content_pct'] +\n",
    "                         ore_data['al2O3_content_pct'] + ore_data['p_content_pct'])\n",
    "    ore_data['ore_quality_index'] = ore_data['mn_grade_pct'] / (ore_data['gangue_total'] + 0.01)\n",
    "    ore_data['mn_to_fe_ratio'] = ore_data['mn_grade_pct'] / (ore_data['fe_content_pct'] + 0.01)\n",
    "    ore_data['mn_to_silica_ratio'] = ore_data['mn_grade_pct'] / (ore_data['siO2_content_pct'] + 0.01)\n",
    "    ore_data['mn_to_al_ratio'] = ore_data['mn_grade_pct'] / (ore_data['al2O3_content_pct'] + 0.01)\n",
    "    ore_data['mn_to_phosphorus_ration'] = ore_data['mn_grade_pct'] / (ore_data['p_content_pct']+ 0.01)\n",
    "    ore_data['valuable_mineral_ratio'] = ore_data['mn_grade_pct'] / (ore_data['mn_grade_pct'] + ore_data['gangue_total'])\n",
    "\n",
    "    # Derived features (8-12)\n",
    "    ore_data['ore_hardness_category'] = pd.cut(ore_data['work_index_kwh_t'],\n",
    "                                         bins=[0, 12, 15, 18, 25],\n",
    "                                         labels=['soft', 'medium', 'hard', 'very_hard'])\n",
    "\n",
    "    ore_data['liberation_difficulty'] = ore_data['work_index_kwh_t'] * ore_data['p80_mm']\n",
    "    ore_data['density_grade_product'] = ore_data['specific_gravity'] * ore_data['mn_grade_pct']\n",
    "    ore_data['moisture_adjusted_grade'] = ore_data['mn_grade_pct'] * (100 - ore_data['moisture_pct']) / 100\n",
    "\n",
    "    max_possible_grade = 52.0\n",
    "    ore_data['enrichment_potential'] = (max_possible_grade - ore_data['mn_grade_pct']) / ore_data['mn_grade_pct']\n",
    "\n",
    "    # Ore type encoding (13-14)\n",
    "    ore_type_dummies = pd.get_dummies(ore_data['ore_type'], prefix='ore_type')\n",
    "    ore_data = pd.concat([ore_data, ore_type_dummies], axis=1)\n",
    "\n",
    "    processability_map = {'oxide': 0.7, 'carbonate': 0.85, 'silicate': 0.9}\n",
    "    ore_data['ore_processability_score'] = ore_data['ore_type'].map(processability_map)\n",
    "    ore_data['ore_processability_score'] *= (ore_data['mn_grade_pct'] / 50) * (1 / (ore_data['work_index_kwh_t'] / 15))\n",
    "\n",
    "    # Statistical features (15-18)\n",
    "    mean_grade = ore_data['mn_grade_pct'].mean()\n",
    "    ore_data['grade_deviation_from_mean'] = ore_data['mn_grade_pct'] - mean_grade\n",
    "    ore_data['grade_percentile_rank'] = ore_data['mn_grade_pct'].rank(pct=True)\n",
    "    ore_data['is_high_grade'] = (ore_data['mn_grade_pct'] > 60).astype(int)\n",
    "    ore_data['is_low_grade'] = (ore_data['mn_grade_pct'] < 45).astype(int)\n",
    "\n",
    "    print(f\"  Generated {len([c for c in ore_data.columns if c not in ore_df.columns])} ore features\")\n",
    "    return ore_data\n",
    "\n",
    "\n",
    "engineered_datasets['engineered_ore_feed'] = engineer_ore_features(datasets['ore_feed'])"
   ],
   "id": "da59e05781c81d6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Engineering Ore Characteristics Features...\n",
      "  Generated 22 ore features\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### ORE FEATURE CHARACTERISTICS\n",
    "- This feature engineering function for manganese ore begins by creating basic transformations of the manganese grade to capture non-linear relationships, namely mn_grade_squared and mn_grade_log, which can enhance predictive models by emphasizing extreme values or reducing skew.\n",
    "- It then calculates gangue_total as the sum of impurities (Fe, SiO₂, Al₂O₃, P) to measure overall dilution of the ore and derives quality and ratio-based features including ore_quality_index, mn_to_fe_ratio, mn_to_silica_ratio, mn_to_al_ratio, mn_to_phosphorus_ration, and valuable_mineral_ratio; these standardize manganese content relative to impurities, highlighting metallurgical value and potential separation efficiency.\n",
    "- The function also generates derived physical and operational features: ore_hardness_category categorizes ore hardness based on work index into soft, medium, hard, and very hard classes, liberation_difficulty combines hardness with particle size (p80_mm) to indicate processing effort, density_grade_product multiplies specific gravity by Mn grade to capture separation behavior, moisture_adjusted_grade corrects Mn grade for moisture content, and enrichment_potential estimates the potential to upgrade the ore toward a theoretical maximum grade.\n",
    "- To account for ore type effects, categorical encoding is applied with dummy variables (ore_type_oxide, ore_type_carbonate, ore_type_silicate) and a combined ore_processability_score integrates ore type, grade, and hardness into a numerical measure of processing ease.\n",
    "- Finally, statistical features are calculated to capture relative quality across the dataset: grade_deviation_from_mean measures deviation from average Mn grade, grade_percentile_rank ranks samples by percentile, and binary indicators is_high_grade and is_low_grade flag ores with exceptionally high or low Mn content.\n",
    "- Collectively, these engineered features provide a rich, multi-faceted representation of ore chemistry, physical properties, and processing behavior, making the dataset much more informative for downstream modeling, optimization, and metallurgical decision-making."
   ],
   "id": "1ed23465ea95d4db"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T04:49:51.808814Z",
     "start_time": "2025-10-08T04:49:51.742169Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# CATEGORY 2: CRUSHING & SIZE REDUCTION FEATURES\n",
    "def engineer_crushing_features(crushing_df):\n",
    "    \"\"\"Generate crushing circuit features (Features 22-38)\"\"\"\n",
    "    print(\"\\nEngineering Crushing Circuit Features...\")\n",
    "\n",
    "    crushing_data = crushing_df.copy()\n",
    "\n",
    "    # Energy features (22-25)\n",
    "    crushing_data ['energy_per_ton'] = crushing_data ['power_draw_kw'] / (crushing_data ['feed_rate_tph'] + 0.01)\n",
    "    crushing_data ['specific_energy'] = crushing_data ['energy_per_ton'] / (crushing_data ['ore_hardness_wi'] + 0.01)\n",
    "\n",
    "    theoretical_energy = crushing_data ['ore_hardness_wi'] * (\n",
    "        1 / np.sqrt(crushing_data ['product_p80_mm']) - 1 / np.sqrt(50)\n",
    "    )\n",
    "    crushing_data ['energy_efficiency_index'] = theoretical_energy / (crushing_data ['energy_per_ton'] + 0.01)\n",
    "\n",
    "    max_crusher_capacity = 150\n",
    "    crushing_data ['power_utilization'] = crushing_data ['power_draw_kw'] / (max_crusher_capacity * 5)\n",
    "\n",
    "    # Size reduction features (26-28)\n",
    "    crushing_data ['reduction_ratio'] = 50 / (crushing_data ['product_p80_mm'] + 0.01)\n",
    "    crushing_data ['size_reduction_efficiency'] = crushing_data ['reduction_ratio'] / (crushing_data ['energy_per_ton'] + 0.01)\n",
    "    crushing_data ['crushing_effectiveness'] = (50 - crushing_data ['product_p80_mm']) / (crushing_data ['power_draw_kw'] + 0.01)\n",
    "\n",
    "    # Equipment condition features (29-32)\n",
    "    crushing_data ['liner_wear_impact'] = (100 - crushing_data ['liner_wear_pct']) / 100\n",
    "    baseline_vibration = 2.0\n",
    "    crushing_data ['vibration_normalized'] = crushing_data ['vibration_rms_mm_s'] / baseline_vibration\n",
    "\n",
    "    crushing_data ['vibration_severity_category'] = pd.cut(crushing_data ['vibration_rms_mm_s'],\n",
    "                                               bins=[0, 3, 5, 8, 20],\n",
    "                                               labels=['low', 'medium', 'high', 'critical'])\n",
    "\n",
    "    crushing_data ['wear_rate_per_hour'] = crushing_data ['liner_wear_pct'] / (crushing_data .index + 1)\n",
    "\n",
    "    # Operational features (33-36)\n",
    "    max_gap = 25\n",
    "    crushing_data ['gap_utilization'] = crushing_data ['crusher_gap_mm'] / max_gap\n",
    "    crushing_data ['throughput_efficiency'] = crushing_data ['feed_rate_tph'] / max_crusher_capacity\n",
    "    crushing_data ['ore_hardness_interaction'] = crushing_data ['feed_rate_tph'] * crushing_data ['ore_hardness_wi']\n",
    "    crushing_data ['moisture_impact_factor'] = 1 - (crushing_data ['feed_moisture_pct'] / 20)\n",
    "\n",
    "    # Time-based features (37-38)\n",
    "    crushing_data ['hours_since_maintenance'] = (100 - crushing_data ['liner_wear_pct']) * 5\n",
    "    crushing_data ['is_end_of_liner_life'] = (crushing_data ['liner_wear_pct'] < 30).astype(int)\n",
    "\n",
    "    print(f\"  Generated {len([c for c in crushing_data.columns if c not in crushing_df.columns])} crushing features\")\n",
    "    return crushing_data\n",
    "\n",
    "engineered_datasets['engineered_crushing'] = engineer_crushing_features(datasets['crushing'])\n"
   ],
   "id": "8d780eebefe59414",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Engineering Crushing Circuit Features...\n",
      "  Generated 17 crushing features\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### CRUSHING AND SIZE REDUCTION FEATURES\n",
    "- The engineer_crushing_features function systematically generates 17 new features (Features 22–38) designed to quantify the performance, efficiency, and operational health of the crushing circuit.\n",
    "- It starts by calculating energy-related features such as energy_per_ton, which measures the power consumption per ton of ore to highlight energy intensity, and specific_energy, which normalizes this energy by ore hardness to reflect the relative difficulty of crushing harder ores. The energy_efficiency_index compares actual energy usage to the theoretically required energy, providing a measure of operational efficiency, while power_utilization quantifies the fraction of maximum crusher capacity being used. Next, size reduction features like reduction_ratio capture the extent of size reduction, size_reduction_efficiency measures how effectively energy is converted into particle size reduction, and crushing_effectiveness assesses reduction achieved per kilowatt, reflecting performance optimization.\n",
    "- To monitor equipment health, features such as liner_wear_impact indicate remaining liner effectiveness, vibration_normalized scales observed vibrations relative to a baseline to detect anomalies, vibration_severity_category classifies vibration levels into low, medium, high, and critical bands, and wear_rate_per_hour tracks the rate of liner degradation over time.\n",
    "- Operational features including gap_utilization and throughput_efficiency evaluate how effectively the crusher gap and feed rate are being used, while ore_hardness_interaction combines ore hardness and feed rate to estimate mechanical stress, and moisture_impact_factor adjusts performance expectations based on feed moisture content.\n",
    "- Finally, time-based features such as hours_since_maintenance estimate operational hours remaining before maintenance, and is_end_of_liner_life flags liners approaching end-of-life, helping to schedule preventive maintenance and avoid unplanned downtime. Collectively, these features provide a multidimensional view of crushing circuit efficiency, energy consumption, equipment health, and operational performance, enabling better process monitoring, optimization, and predictive modeling.\n",
    "\n",
    "- List of engineered crushing features: energy_per_ton, specific_energy, energy_efficiency_index, power_utilization, reduction_ratio, size_reduction_efficiency, crushing_effectiveness, liner_wear_impact, vibration_normalized, vibration_severity_category, wear_rate_per_hour, gap_utilization, throughput_efficiency, ore_hardness_interaction, moisture_impact_factor, hours_since_maintenance, is_end_of_liner_life."
   ],
   "id": "61927d61abe73e88"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T04:49:52.034065Z",
     "start_time": "2025-10-08T04:49:51.940235Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# CATEGORY 3: SEPARATION CIRCUIT FEATURES\n",
    "def engineer_separation_features(separation_df):\n",
    "    \"\"\"Generate separation circuit features (Features 39-56)\"\"\"\n",
    "    print(\"\\nEngineering Separation Circuit Features...\")\n",
    "\n",
    "    separation_data = separation_df.copy()\n",
    "\n",
    "    # Recovery features (39-42)\n",
    "    theoretical_max = 0.95\n",
    "    separation_data['recovery_efficiency_ratio'] = separation_data['spiral_recovery'] / theoretical_max\n",
    "    separation_data['grade_recovery_product'] = separation_data['spiral_concentrate_grade_pct'] * separation_data['spiral_recovery']\n",
    "\n",
    "    separation_data['separation_sharpness'] = (\n",
    "            (separation_data['spiral_concentrate_grade_pct'] - separation_data['feed_grade_pct']) /\n",
    "            (separation_data['spiral_concentrate_grade_pct'] - separation_data['spiral_tailings_grade_pct'] + 0.01)\n",
    "    )\n",
    "\n",
    "    separation_data['upgrade_ratio'] = separation_data['spiral_concentrate_grade_pct'] / (separation_data['feed_grade_pct'] + 0.01)\n",
    "\n",
    "    # Operational features (43-46)\n",
    "    optimal_spiral_speed = 200\n",
    "    separation_data['spiral_speed_deviation'] = separation_data['spiral_speed_rpm'] - optimal_spiral_speed\n",
    "    separation_data['spiral_speed_squared'] = separation_data['spiral_speed_rpm'] ** 2\n",
    "\n",
    "    separation_data['water_to_solids_ratio'] = separation_data['wash_water_m3h'] / (separation_data['feed_density_pct_solids'] / 100 + 0.01)\n",
    "    separation_data['dilution_factor'] = 100 / (separation_data['feed_density_pct_solids'] + 0.01)\n",
    "\n",
    "    # Performance features (47-49)\n",
    "    separation_data['separation_selectivity'] = (\n",
    "            (separation_data['spiral_concentrate_grade_pct'] / (separation_data['feed_grade_pct'] + 0.01)) /\n",
    "            (separation_data['spiral_recovery'] + 0.01)\n",
    "    )\n",
    "\n",
    "    separation_data['manganese_loss_to_tailings'] = separation_data['spiral_tailings_grade_pct'] * (1 - separation_data['spiral_recovery'])\n",
    "    separation_data['enrichment_index'] = (\n",
    "            (separation_data['spiral_concentrate_grade_pct'] - separation_data['spiral_tailings_grade_pct']) /\n",
    "            (separation_data['feed_grade_pct'] + 0.01)\n",
    "    )\n",
    "\n",
    "    # Magnetic separation features (50-53)\n",
    "    baseline_intensity = 0.8\n",
    "    separation_data['magnetic_intensity_effect'] = separation_data['magnetic_intensity_t'] - baseline_intensity\n",
    "\n",
    "    optimal_belt_speed = 1.0\n",
    "    separation_data['belt_speed_optimal_deviation'] = np.abs(separation_data['belt_speed_ms'] - optimal_belt_speed)\n",
    "\n",
    "    if 'ore_type' in separation_data.columns:\n",
    "        ore_magnetic_map = {'oxide': 0.75, 'carbonate': 0.85, 'silicate': 0.90}\n",
    "        separation_data['magnetic_efficiency_by_ore'] = separation_data['ore_type'].map(ore_magnetic_map)\n",
    "        separation_data['magnetic_susceptibility_proxy'] = separation_data['magnetic_efficiency_by_ore'] * separation_data['feed_grade_pct']\n",
    "\n",
    "    # Combined performance (54-56)\n",
    "    separation_data['overall_enrichment'] = separation_data['final_concentrate_grade_pct'] / (separation_data['feed_grade_pct'] + 0.01)\n",
    "    separation_data['two_stage_recovery'] = separation_data['spiral_recovery'] * separation_data['overall_recovery']\n",
    "\n",
    "    separation_data['spiral_efficiency'] = separation_data['spiral_recovery']\n",
    "    mag_efficiency = separation_data['overall_recovery'] / (separation_data['spiral_recovery'] + 0.01)\n",
    "    separation_data['combined_efficiency'] = (separation_data['spiral_efficiency'] + mag_efficiency) / 2\n",
    "\n",
    "    print(f\"  Generated {len([c for c in separation_data.columns if c not in separation_df.columns])} separation features\")\n",
    "    return separation_data\n",
    "\n",
    "\n",
    "engineered_datasets['engineered_separation'] = engineer_separation_features(datasets['separation'])"
   ],
   "id": "4b24b9744fa17ca5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Engineering Separation Circuit Features...\n",
      "  Generated 19 separation features\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### ENGINEERED SEPARATION FEATURES\n",
    "- The engineer_separation_features function is designed to extract 18 new features (Features 39–56) that describe the recovery performance, operational conditions, and magnetic separation efficiency of the spiral and magnetic separation circuits. The goal is to quantify how effectively ore is separated into concentrate and tailings while considering equipment settings, ore characteristics, and process limitations.\n",
    "- Recovery features include recovery_efficiency_ratio, which compares actual spiral recovery to a theoretical maximum to indicate process efficiency; grade_recovery_product, which combines grade and recovery to measure overall recovery value; separation_sharpness, which quantifies how distinctly valuable minerals are separated from gangue; and upgrade_ratio, representing grade improvement from feed to concentrate.\n",
    "- Operational features monitor process settings, such as spiral_speed_deviation and spiral_speed_squared, which capture deviations from optimal speed and non-linear effects of speed; water_to_solids_ratio and dilution_factor measure slurry consistency, impacting separation performance. Performance metrics include separation_selectivity, which normalizes enrichment relative to recovery; manganese_loss_to_tailings, indicating ore lost in tailings; and enrichment_index, measuring grade improvement relative to feed.\n",
    "- Magnetic separation features such as magnetic_intensity_effect, belt_speed_optimal_deviation, magnetic_efficiency_by_ore, and magnetic_susceptibility_proxy account for magnetic field strength, belt speed, and ore-type sensitivity, improving predictions of magnetic separation effectiveness.\n",
    "- Finally, combined performance metrics like overall_enrichment, two_stage_recovery, spiral_efficiency, and combined_efficiency integrate spiral and magnetic separation results into holistic indicators of circuit efficiency. These engineered features enable deeper analysis of recovery optimization, energy usage, and ore-specific processing behavior, providing a robust dataset for monitoring, modeling, and predictive analytics.\n",
    "\n",
    "- List of engineered separation features: recovery_efficiency_ratio, grade_recovery_product, separation_sharpness, upgrade_ratio, spiral_speed_deviation, spiral_speed_squared, water_to_solids_ratio, dilution_factor, separation_selectivity, manganese_loss_to_tailings, enrichment_index, magnetic_intensity_effect, belt_speed_optimal_deviation, magnetic_efficiency_by_ore, magnetic_susceptibility_proxy, overall_enrichment, two_stage_recovery, spiral_efficiency, combined_efficiency."
   ],
   "id": "16af6dff1429b7db"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T04:49:52.209396Z",
     "start_time": "2025-10-08T04:49:52.106060Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# CATEGORY 4: FLOTATION CIRCUIT FEATURES\n",
    "def engineer_flotation_features(flotation_df):\n",
    "    \"\"\"Generate flotation circuit features (Features 57-79)\"\"\"\n",
    "    print(\"\\nEngineering Flotation Circuit Features...\")\n",
    "\n",
    "    floatation_data = flotation_df.copy()\n",
    "\n",
    "    # Reagent optimization - Dosage features (57-61)\n",
    "    floatation_data['collector_intensity'] = floatation_data['collector_dosage_gt'] / (floatation_data['feed_grade_pct'] + 0.01)\n",
    "    floatation_data['frother_intensity'] = floatation_data['frother_dosage_gt'] / (floatation_data['pulp_density_pct_solids'] + 0.01)\n",
    "\n",
    "    floatation_data['reagent_cost_per_ton'] = (\n",
    "                                                      floatation_data['collector_dosage_gt'] * 0.5 + floatation_data['frother_dosage_gt'] * 0.8\n",
    "    ) / 1000\n",
    "\n",
    "    floatation_data['collector_to_frother_ratio'] = floatation_data['collector_dosage_gt'] / (floatation_data['frother_dosage_gt'] + 0.01)\n",
    "    floatation_data['reagent_efficiency'] = floatation_data['flotation_recovery'] / (\n",
    "            floatation_data['collector_dosage_gt'] + floatation_data['frother_dosage_gt'] + 0.01\n",
    "    )\n",
    "\n",
    "    # Dosing accuracy features (62-64)\n",
    "    if 'actual_collector_consumed_gt' in floatation_data.columns:\n",
    "        floatation_data['dosing_error'] = floatation_data['actual_collector_consumed_gt'] - floatation_data['collector_dosage_gt']\n",
    "        floatation_data['reagent_wastage'] = floatation_data['dosing_error'] / (floatation_data['collector_dosage_gt'] + 0.01)\n",
    "\n",
    "        if 'blower_health_score' in floatation_data.columns:\n",
    "            floatation_data['pump_health_impact_on_dosing'] = floatation_data['dosing_error'] * (1 - floatation_data['blower_health_score'] / 100)\n",
    "\n",
    "    # pH features (65-68)\n",
    "    optimal_ph = 9.25\n",
    "    floatation_data['ph_deviation_from_optimal'] = np.abs(floatation_data['ph_value'] - optimal_ph)\n",
    "    floatation_data['ph_in_optimal_range'] = ((floatation_data['ph_value'] >= 9.0) & (floatation_data['ph_value'] <= 9.5)).astype(int)\n",
    "    floatation_data['ph_recovery_interaction'] = floatation_data['ph_value'] * floatation_data['flotation_recovery']\n",
    "    floatation_data['ph_squared'] = floatation_data['ph_value'] ** 2\n",
    "\n",
    "    # Process features (69-72)\n",
    "    floatation_data['air_to_solids_ratio'] = floatation_data['air_flow_m3_min'] / (floatation_data['pulp_density_pct_solids'] + 0.01)\n",
    "    floatation_data['residence_time_per_grade'] = floatation_data['residence_time_min'] / (floatation_data['feed_grade_pct'] + 0.01)\n",
    "    floatation_data['flotation_kinetics_factor'] = floatation_data['residence_time_min'] * floatation_data['air_flow_m3_min']\n",
    "    floatation_data['froth_loading'] = floatation_data['concentrate_grade_pct'] / (floatation_data['froth_stability_index'] + 0.01)\n",
    "\n",
    "    # Equipment-linked performance (73-76)\n",
    "    if 'cell_health_score' in floatation_data.columns:\n",
    "        floatation_data['cell_health_recovery_product'] = floatation_data['cell_health_score'] * floatation_data['flotation_recovery']\n",
    "        floatation_data['equipment_degradation_impact'] = (100 - floatation_data['cell_health_score']) * floatation_data.get('actual_collector_consumed_gt', floatation_data['collector_dosage_gt']) / 100\n",
    "\n",
    "    if 'blower_health_score' in floatation_data.columns:\n",
    "        floatation_data['blower_efficiency_factor'] = floatation_data['blower_health_score'] / 100\n",
    "\n",
    "    floatation_data['agitator_mixing_efficiency'] = floatation_data.get('cell_health_score', 80) * floatation_data['flotation_recovery'] / 100\n",
    "\n",
    "    # Ore type interactions (77-79)\n",
    "    if 'ore_type' in floatation_data.columns:\n",
    "        ore_flotation_map = {'oxide': 0.65, 'carbonate': 0.78, 'silicate': 0.85}\n",
    "        floatation_data['flotation_ore_suitability'] = floatation_data['ore_type'].map(ore_flotation_map)\n",
    "\n",
    "        floatation_data['carbonate_flotation_bonus'] = (\n",
    "                (floatation_data['ore_type'] == 'carbonate').astype(int) * floatation_data['flotation_recovery'] * 0.1\n",
    "        )\n",
    "\n",
    "        floatation_data['oxide_flotation_penalty'] = (\n",
    "                (floatation_data['ore_type'] == 'oxide').astype(int) * floatation_data['flotation_recovery'] * 0.15\n",
    "        )\n",
    "\n",
    "    print(f\"  Generated {len([c for c in floatation_data.columns if c not in flotation_df.columns])} flotation features\")\n",
    "    return floatation_data\n",
    "\n",
    "engineered_datasets['engineered_flotation'] = engineer_flotation_features(datasets['flotation'])\n"
   ],
   "id": "17f3db4080835e55",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Engineering Flotation Circuit Features...\n",
      "  Generated 23 flotation features\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T04:49:52.333321Z",
     "start_time": "2025-10-08T04:49:52.285258Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# CATEGORY 5: DMS FEATURES\n",
    "def engineer_dms_features(dms_df):\n",
    "    \"\"\"Generate DMS circuit features (Features 80-92)\"\"\"\n",
    "    print(\"\\nEngineering DMS Circuit Features...\")\n",
    "\n",
    "    dms_data = dms_df.copy()\n",
    "\n",
    "    # Media properties (80-82)\n",
    "    dms_data['density_differential'] = np.abs(dms_data['ore_density_sg'] - dms_data['media_density_sg'])\n",
    "    dms_data['separation_sharpness_dms'] = dms_data['density_differential'] / 0.5\n",
    "    dms_data['media_efficiency'] = dms_data['media_recovery_pct'] / 100\n",
    "\n",
    "    # Cyclone performance (83-86)\n",
    "    if 'cyclone_health_score' in dms_data.columns:\n",
    "        dms_data['cyclone_health_efficiency_product'] = dms_data['cyclone_health_score'] * dms_data['separation_efficiency']\n",
    "\n",
    "    if 'cyclone_wear_rate_pct' in dms_data.columns:\n",
    "        dms_data['wear_impact_on_separation'] = dms_data['separation_efficiency'] * (100 - dms_data['cyclone_wear_rate_pct']) / 100\n",
    "\n",
    "    max_pressure = 120\n",
    "    dms_data['pressure_utilization'] = dms_data['cyclone_pressure_kpa'] / max_pressure\n",
    "    dms_data['media_consumption_efficiency'] = dms_data['dms_recovery'] / (dms_data['media_consumption_kg_t'] + 0.01)\n",
    "\n",
    "    # Size effects (87-89)\n",
    "    dms_data['size_suitability_for_dms'] = (dms_data['feed_size_mm'] >= 10).astype(int)\n",
    "    optimal_size = 25\n",
    "    dms_data['coarse_fraction_ratio'] = dms_data['feed_size_mm'] / optimal_size\n",
    "    dms_data['size_density_interaction'] = dms_data['feed_size_mm'] * dms_data['density_differential']\n",
    "\n",
    "    # Product quality (90-92)\n",
    "    dms_data['sink_float_separation'] = dms_data['sink_grade_pct'] / (dms_data['float_grade_pct'] + 0.01)\n",
    "    dms_data['yield_recovery_product'] = (dms_data['sink_yield_pct'] / 100) * dms_data['dms_recovery']\n",
    "    dms_data['dms_upgrade_factor'] = dms_data['sink_grade_pct'] / (dms_data['feed_grade_pct'] + 0.01)\n",
    "\n",
    "    print(f\"  Generated {len([c for c in dms_data.columns if c not in dms_df.columns])} DMS features\")\n",
    "    return dms_data\n",
    "\n",
    "\n",
    "engineered_datasets['engineered_dms'] = engineer_dms_features(datasets['dms'])"
   ],
   "id": "2a93fd5d15c13955",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Engineering DMS Circuit Features...\n",
      "  Generated 13 DMS features\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T04:49:52.485784Z",
     "start_time": "2025-10-08T04:49:52.437872Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# CATEGORY 6: JIGGING FEATURES\n",
    "def engineer_jigging_features(jigging_df):\n",
    "    \"\"\"Generate jigging circuit features (Features 93-103)\"\"\"\n",
    "    print(\"\\nEngineering Jigging Circuit Features...\")\n",
    "\n",
    "    jigging_data = jigging_df.copy()\n",
    "\n",
    "    # Stroke optimization (93-95)\n",
    "    jigging_data['stroke_work_index'] = jigging_data['stroke_length_mm'] * jigging_data['stroke_frequency_spm'] / 1000\n",
    "    optimal_stroke_work = 4.0\n",
    "    jigging_data['stroke_deviation_from_optimal'] = np.abs(jigging_data['stroke_work_index'] - optimal_stroke_work)\n",
    "    jigging_data['stroke_efficiency'] = 1 - (jigging_data['stroke_deviation_from_optimal'] / optimal_stroke_work)\n",
    "\n",
    "    # Hydraulic features (96-98)\n",
    "    jigging_data['water_to_bed_ratio'] = jigging_data['water_flow_m3h_m2'] / (jigging_data['bed_height_mm'] + 0.01)\n",
    "    jigging_data['hutch_water_efficiency'] = jigging_data['jig_recovery'] / (jigging_data['hutch_water_m3h'] + 0.01)\n",
    "    jigging_data['hydraulic_regime'] = jigging_data['water_flow_m3h_m2'] * jigging_data['stroke_frequency_spm']\n",
    "\n",
    "    # Stratification features (99-101)\n",
    "    optimal_feed_size = 20\n",
    "    jigging_data['particle_size_jigging_factor'] = jigging_data['feed_size_mm'] / optimal_feed_size\n",
    "    jigging_data['density_stratification_potential'] = jigging_data.get('ore_density_sg', 4.0) - 2.5\n",
    "    optimal_bed_height = 225\n",
    "    jigging_data['bed_height_optimization'] = jigging_data['bed_height_mm'] / optimal_bed_height\n",
    "\n",
    "    # Equipment health impact (102-103)\n",
    "    if 'jig_health_score' in jigging_data.columns:\n",
    "        jigging_data['jig_health_separation_product'] = jigging_data['jig_health_score'] * jigging_data['separation_efficiency']\n",
    "\n",
    "    if 'jig_wear_rate_pct' in jigging_data.columns:\n",
    "        jigging_data['screen_wear_impact'] = jigging_data['jig_recovery'] * (100 - jigging_data['jig_wear_rate_pct']) / 100\n",
    "\n",
    "    print(f\"  Generated {len([c for c in jigging_data.columns if c not in jigging_df.columns])} jigging features\")\n",
    "    return jigging_data\n",
    "\n",
    "engineered_datasets['engineered_jigging'] = engineer_jigging_features(datasets['jigging'])"
   ],
   "id": "9941242bd9b0337b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Engineering Jigging Circuit Features...\n",
      "  Generated 11 jigging features\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T04:49:52.606Z",
     "start_time": "2025-10-08T04:49:52.554341Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# CATEGORY 7: DEWATERING FEATURES\n",
    "def engineer_dewatering_features(dewatering_df):\n",
    "    \"\"\"Generate dewatering circuit features (Features 104-115)\"\"\"\n",
    "    print(\"\\nEngineering Dewatering Circuit Features...\")\n",
    "\n",
    "    dewatering_data = dewatering_df.copy()\n",
    "\n",
    "    # Thickening features (104-108)\n",
    "    dewatering_data['thickening_ratio'] = dewatering_data['underflow_solids_pct'] / (dewatering_data['feed_solids_pct'] + 0.01)\n",
    "    dewatering_data['flocculant_efficiency'] = dewatering_data['thickening_efficiency'] / (dewatering_data['flocculant_dosage_gt'] + 0.01)\n",
    "    optimal_retention = 4.0\n",
    "    dewatering_data['retention_adequacy'] = dewatering_data['retention_time_hr'] / optimal_retention\n",
    "    dewatering_data['overflow_quality_index'] = 1 / (dewatering_data['overflow_clarity_ntu'] + 1)\n",
    "\n",
    "    if 'thickener_health_score' in dewatering_data.columns:\n",
    "        dewatering_data['thickener_health_impact'] = dewatering_data['thickening_efficiency'] * dewatering_data['thickener_health_score'] / 100\n",
    "\n",
    "    # Filtration features (109-113)\n",
    "    dewatering_data['filter_pressure_efficiency'] = (400 - dewatering_data['filter_pressure_kpa']) / 200\n",
    "    dewatering_data['moisture_reduction'] = dewatering_data['feed_solids_pct'] - (100 - dewatering_data['cake_moisture_pct'])\n",
    "    optimal_cycle = 67.5\n",
    "    dewatering_data['cycle_time_efficiency'] = optimal_cycle / (dewatering_data['cycle_time_min'] + 0.01)\n",
    "\n",
    "    if 'filter_health_score' in dewatering_data.columns:\n",
    "        dewatering_data['filter_health_moisture_impact'] = dewatering_data['cake_moisture_pct'] * (100 - dewatering_data['filter_health_score']) / 100\n",
    "\n",
    "    dewatering_data['dewatering_energy_intensity'] = dewatering_data['filter_pressure_kpa'] * dewatering_data['cycle_time_min']\n",
    "\n",
    "    # Recovery features (114-116)\n",
    "    dewatering_data['water_recovery_efficiency'] = dewatering_data['water_recovery_pct'] / 100\n",
    "    dewatering_data['solid_loss'] = 100 - dewatering_data['solid_recovery_pct']\n",
    "    dewatering_data['overall_dewatering_efficiency'] = (\n",
    "        dewatering_data['thickening_efficiency'] + dewatering_data['water_recovery_pct']/100 + dewatering_data['solid_recovery_pct']/100\n",
    "    ) / 3\n",
    "\n",
    "    print(f\"  Generated {len([c for c in dewatering_data.columns if c not in dewatering_df.columns])} dewatering features\")\n",
    "    return dewatering_data\n",
    "\n",
    "engineered_datasets['engineered_dewatering'] = engineer_dewatering_features(datasets['dewatering'])"
   ],
   "id": "aae02690fe29d2d7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Engineering Dewatering Circuit Features...\n",
      "  Generated 13 dewatering features\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T04:49:52.761180Z",
     "start_time": "2025-10-08T04:49:52.679026Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# CATEGORY 8: EQUIPMENT HEALTH FEATURES\n",
    "def engineer_equipment_features(equipment_df):\n",
    "    \"\"\"Generate equipment health features (Features 117-129)\"\"\"\n",
    "    print(\"\\nEngineering Equipment Health Features...\")\n",
    "\n",
    "    equipment_data = equipment_df.copy()\n",
    "\n",
    "    # General equipment features (117-121)\n",
    "    equipment_data['health_degradation_rate'] = (100 - equipment_data['health_score']) / (equipment_data['operating_hours'] + 1)\n",
    "\n",
    "    equipment_data['failure_risk_category'] = pd.cut(equipment_data['failure_probability'],\n",
    "                                         bins=[0, 0.1, 0.3, 0.5, 1.0],\n",
    "                                         labels=['low', 'medium', 'high', 'critical'])\n",
    "\n",
    "    equipment_data['maintenance_urgency_score'] = equipment_data['failure_probability'] * (1 / (equipment_data['rul_days'] + 1))\n",
    "\n",
    "    expected_life = 87600  # 10 years in hours\n",
    "    equipment_data['equipment_age_factor'] = equipment_data['operating_hours'] / expected_life\n",
    "\n",
    "    if 'wear_rate_pct' in equipment_data.columns:\n",
    "        equipment_data['health_wear_product'] = equipment_data['health_score'] * (100 - equipment_data['wear_rate_pct']) / 100\n",
    "\n",
    "    # Vibration analysis (122-124)\n",
    "    acceptable_vibration = 5.0\n",
    "    equipment_data['vibration_severity_index'] = equipment_data['vibration_rms'] / acceptable_vibration\n",
    "    equipment_data['vibration_health_ratio'] = equipment_data['vibration_rms'] / (equipment_data['health_score'] / 10 + 0.01)\n",
    "    equipment_data['is_vibration_critical'] = (equipment_data['vibration_rms'] > 10).astype(int)\n",
    "\n",
    "    # Thermal features (125-127)\n",
    "    normal_temp_map = {\n",
    "        'crusher': 65, 'pump': 75, 'flotation': 45,\n",
    "        'magnetic': 55, 'conveyor': 50\n",
    "    }\n",
    "\n",
    "    equipment_data['normal_operating_temp'] = equipment_data['equipment_type'].map(\n",
    "        lambda x: normal_temp_map.get(x.split('_')[0], 50)\n",
    "    )\n",
    "    equipment_data['temperature_deviation'] = equipment_data['temperature_c'] - equipment_data['normal_operating_temp']\n",
    "\n",
    "    max_safe_temp = 120\n",
    "    equipment_data['thermal_stress_index'] = equipment_data['temperature_c'] / max_safe_temp\n",
    "    equipment_data['temperature_health_interaction'] = equipment_data['temperature_c'] * (100 - equipment_data['health_score'])\n",
    "\n",
    "    # Performance degradation (128-130)\n",
    "    equipment_data['power_factor_degradation'] = 0.90 - equipment_data['power_factor']\n",
    "    equipment_data['efficiency_loss_estimate'] = (100 - equipment_data['health_score']) * 0.5\n",
    "\n",
    "    if 'maintenance_priority' in equipment_data.columns:\n",
    "        equipment_data['maintenance_priority_weighted'] = equipment_data['maintenance_priority'] * equipment_data['failure_probability']\n",
    "\n",
    "    print(f\"  Generated {len([c for c in equipment_data.columns if c not in equipment_df.columns])} equipment features\")\n",
    "    return equipment_data\n",
    "\n",
    "engineered_datasets['engineered_equipment'] = engineer_equipment_features(datasets['equipment'])"
   ],
   "id": "7eb1601931b9e0f4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Engineering Equipment Health Features...\n",
      "  Generated 15 equipment features\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T04:49:53.003244Z",
     "start_time": "2025-10-08T04:49:52.836254Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# CATEGORY 9: ENERGY CONSUMPTION FEATURES\n",
    "def engineer_energy_features(energy_df):\n",
    "    \"\"\"Generate energy consumption features (Features 131–155, hybrid detailed + aggregate + intensity).\"\"\"\n",
    "    print(\"\\nEngineering Energy Consumption Features...\")\n",
    "\n",
    "    energy_data = energy_df.copy()\n",
    "\n",
    "    # --- Core Parameters ---\n",
    "    estimated_throughput = 2000 / 24  # 2000 tpd / 24 hours\n",
    "    design_power = 1000  # Reference design capacity (kW)\n",
    "\n",
    "    # --- Validate ---\n",
    "    if 'total_power_kw' not in energy_data.columns:\n",
    "        raise KeyError(\"Expected column 'total_power_kw' not found in energy data.\")\n",
    "\n",
    "    # --- Base Power Calculations ---\n",
    "    energy_data['total_power_per_ton'] = energy_data['total_power_kw'] / estimated_throughput\n",
    "\n",
    "    # --- Define subsystem groups ---\n",
    "    separation_cols = [\n",
    "        'gravity_separation_power_kw',\n",
    "        'magnetic_separation_power_kw',\n",
    "        'flotation_power_kw',\n",
    "        'dms_power_kw'\n",
    "    ]\n",
    "\n",
    "    auxiliary_cols = [\n",
    "        'thickening_power_kw', 'filtration_power_kw', 'pumping_power_kw',\n",
    "        'conveying_power_kw', 'compressed_air_power_kw', 'water_treatment_power_kw'\n",
    "    ]\n",
    "\n",
    "    # --- Aggregated subsystem powers ---\n",
    "    energy_data['total_separation_power_kw'] = energy_data[separation_cols].sum(axis=1)\n",
    "    energy_data['total_auxiliary_power_kw'] = energy_data[auxiliary_cols].sum(axis=1)\n",
    "\n",
    "    # --- Ratios relative to total ---\n",
    "    ratio_features = {\n",
    "        'crushing_power_kw': 'crushing_power_ratio',\n",
    "        'screening_power_kw': 'screening_power_ratio',\n",
    "        'total_separation_power_kw': 'separation_total_ratio',\n",
    "        'total_auxiliary_power_kw': 'auxiliary_total_ratio'\n",
    "    }\n",
    "\n",
    "    # Add individual subsystem ratios\n",
    "    for col in separation_cols + auxiliary_cols:\n",
    "        if col in energy_data.columns:\n",
    "            ratio_features[col] = f'{col}_ratio'\n",
    "\n",
    "    for col, new_name in ratio_features.items():\n",
    "        energy_data[new_name] = energy_data[col] / (energy_data['total_power_kw'] + 0.01)\n",
    "\n",
    "    # --- Energy Intensity Features (kWh per ton) ---\n",
    "    # Convert kW (instantaneous power) into kWh per ton basis for comparison\n",
    "    intensity_features = ['crushing_power_kw', 'screening_power_kw'] + separation_cols + auxiliary_cols\n",
    "\n",
    "    for col in intensity_features:\n",
    "        if col in energy_data.columns:\n",
    "            energy_data[f'{col}_per_ton'] = energy_data[col] / estimated_throughput\n",
    "\n",
    "    # Aggregated intensities\n",
    "    energy_data['total_separation_energy_per_ton'] = (\n",
    "        energy_data['total_separation_power_kw'] / estimated_throughput\n",
    "    )\n",
    "    energy_data['total_auxiliary_energy_per_ton'] = (\n",
    "        energy_data['total_auxiliary_power_kw'] / estimated_throughput\n",
    "    )\n",
    "\n",
    "    # --- Efficiency & Cost Features ---\n",
    "    if 'apparent_power_kva' in energy_data.columns:\n",
    "        energy_data['power_factor_plant'] = (\n",
    "            energy_data['total_power_kw'] / (energy_data['apparent_power_kva'] + 0.01)\n",
    "        )\n",
    "    else:\n",
    "        energy_data['power_factor_plant'] = np.nan\n",
    "\n",
    "    energy_data['operational_efficiency'] = energy_data['operational_factor'] * (\n",
    "        energy_data['total_power_kw'] / design_power\n",
    "    )\n",
    "\n",
    "    energy_data['energy_cost_per_ton'] = (\n",
    "        energy_data['total_power_kw'] * energy_data['energy_cost_kwh'] / estimated_throughput\n",
    "    )\n",
    "\n",
    "    # --- Time-based Features ---\n",
    "    energy_data['timestamp'] = pd.to_datetime(energy_data['timestamp'], errors='coerce')\n",
    "    energy_data['hour_of_day'] = energy_data['timestamp'].dt.hour\n",
    "    energy_data['day_of_week'] = energy_data['timestamp'].dt.dayofweek\n",
    "    energy_data['month'] = energy_data['timestamp'].dt.month\n",
    "\n",
    "    # Cyclical encoding\n",
    "    energy_data['hour_sin'] = np.sin(2 * np.pi * energy_data['hour_of_day'] / 24)\n",
    "    energy_data['hour_cos'] = np.cos(2 * np.pi * energy_data['hour_of_day'] / 24)\n",
    "    energy_data['day_sin'] = np.sin(2 * np.pi * energy_data['day_of_week'] / 7)\n",
    "    energy_data['day_cos'] = np.cos(2 * np.pi * energy_data['day_of_week'] / 7)\n",
    "\n",
    "    # Peak and weekend flags\n",
    "    energy_data['is_peak_hours'] = (\n",
    "        (energy_data['hour_of_day'] >= 9) & (energy_data['hour_of_day'] <= 17)\n",
    "    ).astype(int)\n",
    "    energy_data['is_weekend'] = (energy_data['day_of_week'] >= 5).astype(int)\n",
    "\n",
    "    # Shift classification\n",
    "    energy_data['shift_number'] = pd.cut(\n",
    "        energy_data['hour_of_day'], bins=[-1, 8, 16, 24], labels=[3, 1, 2]\n",
    "    )\n",
    "\n",
    "    # --- Load Management ---\n",
    "    energy_data['base_load_ratio'] = (\n",
    "        energy_data['base_load_kw'] / (energy_data['total_power_kw'] + 0.01)\n",
    "    )\n",
    "    energy_data['variable_load'] = (\n",
    "        energy_data['total_power_kw'] - energy_data['base_load_kw']\n",
    "    )\n",
    "\n",
    "    energy_data['load_factor'] = (\n",
    "        energy_data['total_power_kw'].rolling(window=24, min_periods=1).mean()\n",
    "        / (energy_data['total_power_kw'].rolling(window=24, min_periods=1).max() + 0.01)\n",
    "    )\n",
    "\n",
    "    energy_data['demand_charge_exposure'] = (\n",
    "        energy_data['total_power_kw'] * energy_data['is_peak_hours']\n",
    "    )\n",
    "\n",
    "    print(f\"Generated {len([c for c in energy_data.columns if c not in energy_df.columns])} new engineered energy features\")\n",
    "    return energy_data\n",
    "\n",
    "engineered_datasets['engineered_energy'] = engineer_energy_features(datasets['energy'])"
   ],
   "id": "dfe5c79fbd1bde5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Engineering Energy Consumption Features...\n",
      "Generated 42 new engineered energy features\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T04:49:53.114259Z",
     "start_time": "2025-10-08T04:49:53.030051Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# CATEGORY 10: BLENDED ORE CHARACTERISTICS FEATURES\n",
    "def engineer_ore_features(ore_df):\n",
    "    \"\"\"Generate ore characteristics features (Features 1-18)\"\"\"\n",
    "    print(\"\\nEngineering Ore Characteristics Features...\")\n",
    "\n",
    "    ore_data = ore_df.copy()\n",
    "\n",
    "    # Basic transformations (1-7)\n",
    "    ore_data['mn_grade_squared'] = ore_data['mn_grade_pct'] ** 2\n",
    "    ore_data['mn_grade_log'] = np.log1p(ore_data['mn_grade_pct'])\n",
    "\n",
    "    ore_data['gangue_total'] = (ore_data['fe_content_pct'] + ore_data['siO2_content_pct'] +\n",
    "                         ore_data['al2O3_content_pct'] + ore_data['p_content_pct'])\n",
    "    ore_data['ore_quality_index'] = ore_data['mn_grade_pct'] / (ore_data['gangue_total'] + 0.01)\n",
    "    ore_data['mn_to_fe_ratio'] = ore_data['mn_grade_pct'] / (ore_data['fe_content_pct'] + 0.01)\n",
    "    ore_data['mn_to_silica_ratio'] = ore_data['mn_grade_pct'] / (ore_data['siO2_content_pct'] + 0.01)\n",
    "    ore_data['mn_to_al_ratio'] = ore_data['mn_grade_pct'] / (ore_data['al2O3_content_pct'] + 0.01)\n",
    "    ore_data['mn_to_phosphorus_ration'] = ore_data['mn_grade_pct'] / (ore_data['p_content_pct']+ 0.01)\n",
    "    ore_data['valuable_mineral_ratio'] = ore_data['mn_grade_pct'] / (ore_data['mn_grade_pct'] + ore_data['gangue_total'])\n",
    "\n",
    "    # Derived features (8-12)\n",
    "    ore_data['ore_hardness_category'] = pd.cut(ore_data['work_index_kwh_t'],\n",
    "                                         bins=[0, 12, 15, 18, 25],\n",
    "                                         labels=['soft', 'medium', 'hard', 'very_hard'])\n",
    "\n",
    "    ore_data['liberation_difficulty'] = ore_data['work_index_kwh_t'] * ore_data['p80_mm']\n",
    "    ore_data['density_grade_product'] = ore_data['specific_gravity'] * ore_data['mn_grade_pct']\n",
    "    ore_data['moisture_adjusted_grade'] = ore_data['mn_grade_pct'] * (100 - ore_data['moisture_pct']) / 100\n",
    "\n",
    "    max_possible_grade = 52.0\n",
    "    ore_data['enrichment_potential'] = (max_possible_grade - ore_data['mn_grade_pct']) / ore_data['mn_grade_pct']\n",
    "\n",
    "    # Ore type encoding (13-14)\n",
    "    ore_type_dummies = pd.get_dummies(ore_data['ore_type'], prefix='ore_type')\n",
    "    ore_data = pd.concat([ore_data, ore_type_dummies], axis=1)\n",
    "\n",
    "    processability_map = {'oxide': 0.7, 'carbonate': 0.85, 'silicate': 0.9}\n",
    "    ore_data['ore_processability_score'] = ore_data['ore_type'].map(processability_map)\n",
    "    ore_data['ore_processability_score'] *= (ore_data['mn_grade_pct'] / 50) * (1 / (ore_data['work_index_kwh_t'] / 15))\n",
    "\n",
    "    # Statistical features (15-18)\n",
    "    mean_grade = ore_data['mn_grade_pct'].mean()\n",
    "    ore_data['grade_deviation_from_mean'] = ore_data['mn_grade_pct'] - mean_grade\n",
    "    ore_data['grade_percentile_rank'] = ore_data['mn_grade_pct'].rank(pct=True)\n",
    "    ore_data['is_high_grade'] = (ore_data['mn_grade_pct'] > 60).astype(int)\n",
    "    ore_data['is_low_grade'] = (ore_data['mn_grade_pct'] < 45).astype(int)\n",
    "\n",
    "    print(f\"  Generated {len([c for c in ore_data.columns if c not in ore_df.columns])} blended ore features\")\n",
    "    return ore_data\n",
    "\n",
    "\n",
    "engineered_datasets['engineered_ore_feed'] = engineer_ore_features(datasets['blended_ore'])"
   ],
   "id": "7bd9f880968c3620",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Engineering Ore Characteristics Features...\n",
      "  Generated 22 blended ore features\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T04:49:53.266807Z",
     "start_time": "2025-10-08T04:49:53.160992Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# CATEGORY 11: LAG & ROLLING FEATURES\n",
    "def engineer_lag_rolling_features(df, target_columns, timestamp_col='timestamp'):\n",
    "    \"\"\"\n",
    "    Generate lag, rolling, and change features for time series data (Features 163-174)\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame containing time series data\n",
    "    target_columns : list\n",
    "        List of numeric columns to generate lag/rolling features for\n",
    "    timestamp_col : str\n",
    "        Column representing the timestamp (must exist in df)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    df_sorted : pd.DataFrame\n",
    "        Original DataFrame with new lag, rolling, and change features appended\n",
    "    \"\"\"\n",
    "    print(\"\\nEngineering Lag & Rolling Features...\")\n",
    "\n",
    "    if timestamp_col not in df.columns:\n",
    "        print(\"  Warning: No timestamp column found, skipping time series features\")\n",
    "        return df\n",
    "\n",
    "    # Sort by timestamp\n",
    "    df_sorted = df.sort_values(timestamp_col).copy()\n",
    "\n",
    "    for col in target_columns:\n",
    "        if col not in df_sorted.columns:\n",
    "            print(f\"  Warning: {col} not found in DataFrame, skipping\")\n",
    "            continue\n",
    "\n",
    "        # Lag features (1h and 24h)\n",
    "        df_sorted[f'{col}_lag_1h'] = df_sorted[col].shift(1)\n",
    "        df_sorted[f'{col}_lag_24h'] = df_sorted[col].shift(24)\n",
    "\n",
    "        # Rolling window features (mean and std over 24h)\n",
    "        df_sorted[f'{col}_rolling_mean_24h'] = df_sorted[col].rolling(window=24, min_periods=1).mean()\n",
    "        df_sorted[f'{col}_rolling_std_24h'] = df_sorted[col].rolling(window=24, min_periods=1).std()\n",
    "\n",
    "        # Change / difference features\n",
    "        df_sorted[f'{col}_change_rate'] = df_sorted[col].pct_change()\n",
    "        df_sorted[f'{col}_change_1h'] = df_sorted[col].diff()\n",
    "\n",
    "        # Second derivative (acceleration)\n",
    "        df_sorted[f'{col}_acceleration'] = df_sorted[f'{col}_change_1h'].diff()\n",
    "\n",
    "        # Short-term variability (12h rolling std)\n",
    "        df_sorted[f'{col}_variability_12h'] = df_sorted[col].rolling(window=12, min_periods=1).std()\n",
    "\n",
    "    print(f\"  Generated lag/rolling features for {len(target_columns)} variables\")\n",
    "    return df_sorted\n",
    "\n",
    "\n",
    "target_cols = ['flotation_recovery', 'collector_dosage_gt', 'frother_dosage_gt']\n",
    "engineered_datasets['engineered_flotation_lagged'] = engineer_lag_rolling_features(\n",
    "     df=engineered_datasets['engineered_flotation'],\n",
    "     target_columns=target_cols,\n",
    "     timestamp_col='timestamp'\n",
    ")\n"
   ],
   "id": "bb5bbcc49dfed2f0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Engineering Lag & Rolling Features...\n",
      "  Generated lag/rolling features for 3 variables\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T04:49:54.861537Z",
     "start_time": "2025-10-08T04:49:53.311583Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# CATEGORY 13: CROSS-DATASET INTEGRATION FEATURES (Auto-detect engineered datasets)\n",
    "def engineer_cross_dataset_features_auto(engineered_datasets):\n",
    "    \"\"\"Generate cross-dataset integration features (snapshot, Features 148-162) automatically detecting engineered datasets\"\"\"\n",
    "    print(\"\\nEngineering Cross-Dataset Integration Features (Snapshot)...\")\n",
    "\n",
    "    integrated_features = {}\n",
    "\n",
    "    # Map dataset types from engineered dataset keys\n",
    "    dataset_map = {\n",
    "        'separation': None,\n",
    "        'flotation': None,\n",
    "        'dms': None,\n",
    "        'jigging': None,\n",
    "        'ore_feed': None,\n",
    "        'equipment': None\n",
    "    }\n",
    "\n",
    "    for key in engineered_datasets.keys():\n",
    "        if 'separation' in key:\n",
    "            dataset_map['separation'] = engineered_datasets[key]\n",
    "        elif 'flotation' in key:\n",
    "            dataset_map['flotation'] = engineered_datasets[key]\n",
    "        elif 'dms' in key:\n",
    "            dataset_map['dms'] = engineered_datasets[key]\n",
    "        elif 'jigging' in key:\n",
    "            dataset_map['jigging'] = engineered_datasets[key]\n",
    "        elif 'ore_feed' in key:\n",
    "            dataset_map['ore_feed'] = engineered_datasets[key]\n",
    "        elif 'equipment' in key:\n",
    "            dataset_map['equipment'] = engineered_datasets[key]\n",
    "\n",
    "    # Plant-wide performance metrics\n",
    "    recoveries = []\n",
    "    if dataset_map['separation'] is not None:\n",
    "        recoveries.append(dataset_map['separation']['overall_recovery'].mean())\n",
    "    if dataset_map['flotation'] is not None:\n",
    "        recoveries.append(dataset_map['flotation']['flotation_recovery'].mean())\n",
    "    if dataset_map['dms'] is not None:\n",
    "        recoveries.append(dataset_map['dms']['dms_recovery'].mean())\n",
    "    if dataset_map['jigging'] is not None:\n",
    "        recoveries.append(dataset_map['jigging']['jig_recovery'].mean())\n",
    "\n",
    "    if recoveries:\n",
    "        integrated_features['overall_plant_recovery'] = np.mean(recoveries)\n",
    "        print(f\"  Overall Plant Recovery: {integrated_features['overall_plant_recovery']:.3f}\")\n",
    "\n",
    "    # Total enrichment factor\n",
    "    if dataset_map['ore_feed'] is not None and dataset_map['flotation'] is not None:\n",
    "        original_grade = dataset_map['ore_feed']['mn_grade_pct'].mean()\n",
    "        final_grade = dataset_map['flotation']['concentrate_grade_pct'].mean()\n",
    "        integrated_features['total_enrichment_factor'] = final_grade / original_grade\n",
    "        print(f\"  Total Enrichment Factor: {integrated_features['total_enrichment_factor']:.2f}\")\n",
    "\n",
    "    # Equipment-process correlation\n",
    "    if dataset_map['equipment'] is not None:\n",
    "        equipment_df = dataset_map['equipment']\n",
    "        critical_equipment_types = ['crusher', 'flotation_cell', 'pump', 'magnetic_separator']\n",
    "        critical_mask = equipment_df['equipment_type'].str.contains('|'.join(critical_equipment_types), na=False)\n",
    "\n",
    "        if critical_mask.sum() > 0:\n",
    "            integrated_features['critical_equipment_health_avg'] = equipment_df[critical_mask]['health_score'].mean()\n",
    "            integrated_features['equipment_bottleneck_score'] = equipment_df[critical_mask]['health_score'].min()\n",
    "            integrated_features['equipment_reliability_index'] = 1 - equipment_df['failure_probability'].mean()\n",
    "            high_risk_count = (equipment_df['failure_probability'] > 0.3).sum()\n",
    "            integrated_features['maintenance_burden'] = high_risk_count / len(equipment_df)\n",
    "            print(f\"  Critical Equipment Health: {integrated_features['critical_equipment_health_avg']:.1f}\")\n",
    "\n",
    "    return pd.DataFrame([integrated_features])\n",
    "\n",
    "def engineer_cross_dataset_features_over_time(engineered_datasets, timestamp_col='timestamp', freq='M'):\n",
    "    \"\"\"Generate cross-dataset features aggregated over time (monthly or yearly)\"\"\"\n",
    "    print(f\"\\nEngineering Cross-Dataset Features over time (freq={freq})...\")\n",
    "\n",
    "    # Determine which datasets have timestamps\n",
    "    time_dfs = {}\n",
    "    for key, df in engineered_datasets.items():\n",
    "        if timestamp_col in df.columns:\n",
    "            df_copy = df.copy()\n",
    "            df_copy[timestamp_col] = pd.to_datetime(df_copy[timestamp_col])\n",
    "            df_copy.set_index(timestamp_col, inplace=True)\n",
    "            time_dfs[key] = df_copy\n",
    "\n",
    "    aggregated_features = {}\n",
    "    for key, df in time_dfs.items():\n",
    "        # Keep only numeric columns\n",
    "        numeric_cols = df.select_dtypes(include=np.number).columns\n",
    "        df_numeric = df[numeric_cols].resample(freq).mean().reset_index()\n",
    "        # Add dataset prefix to all numeric columns except timestamp\n",
    "        df_numeric = df_numeric.rename(columns={col: f\"{key}_{col}\" for col in numeric_cols})\n",
    "        aggregated_features[key] = df_numeric\n",
    "\n",
    "    # Merge all aggregated datasets on timestamp\n",
    "    all_features = None\n",
    "    for df in aggregated_features.values():\n",
    "        if all_features is None:\n",
    "            all_features = df\n",
    "        else:\n",
    "            all_features = pd.merge(all_features, df, on=timestamp_col, how='outer')\n",
    "\n",
    "    return all_features\n",
    "\n",
    "\n",
    "# ======= Generate all three =======\n",
    "# Snapshot\n",
    "engineered_datasets['cross_dataset_features_snapshot'] = engineer_cross_dataset_features_auto(engineered_datasets)\n",
    "\n",
    "# Monthly\n",
    "engineered_datasets['cross_dataset_features_monthly'] = engineer_cross_dataset_features_over_time(\n",
    "    engineered_datasets, freq='M'\n",
    ")\n",
    "\n",
    "# Yearly\n",
    "engineered_datasets['cross_dataset_features_yearly'] = engineer_cross_dataset_features_over_time(\n",
    "    engineered_datasets, freq='Y'\n",
    ")\n"
   ],
   "id": "36ac7139dd7494fa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Engineering Cross-Dataset Integration Features (Snapshot)...\n",
      "  Overall Plant Recovery: 0.567\n",
      "  Total Enrichment Factor: 0.92\n",
      "  Critical Equipment Health: 74.5\n",
      "\n",
      "Engineering Cross-Dataset Features over time (freq=M)...\n",
      "\n",
      "Engineering Cross-Dataset Features over time (freq=Y)...\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### CROSS DATASET FEATURES OVERTIME:\n",
    "\n",
    "- In this stage, we engineered Category 13: Cross-Dataset Integration Features, designed to capture plant-wide performance insights by combining information from multiple processing stages and equipment datasets rather than analyzing them in isolation. The goal was to create high-level KPIs that summarize the operational efficiency, reliability, and productivity of the entire manganese processing plant.\n",
    "- We achieved this by dynamically linking datasets such as ore feed, flotation, separation, DMS, jigging, and equipment, then computing metrics that reflect their interdependencies. Specifically, we engineered features including overall_plant_recovery, representing the mean recovery across all key beneficiation processes; total_enrichment_factor, quantifying the ratio of final concentrate grade to original feed grade (a measure of processing effectiveness); critical_equipment_health_avg, reflecting the mean condition of essential machines like crushers and flotation cells; equipment_bottleneck_score, identifying the weakest-performing equipment unit; equipment_reliability_index, showing the average operational reliability based on failure probabilities; and maintenance_burden, estimating the proportion of high-risk equipment requiring frequent intervention.\n",
    "- To make these KPIs more actionable, we extended the function to compute them **over time—monthly or yearly—**enabling trend analysis and performance tracking.\n",
    "- This time-aware design is particularly significant because it allows engineers and analysts to monitor process efficiency, detect declines in performance, and plan maintenance or process adjustments proactively.\n",
    "- In essence, these integrated features transform raw process data into strategic, interpretable performance indicators, forming the backbone of intelligent plant optimization and decision support systems."
   ],
   "id": "21d0918a5eb80132"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T04:49:55.028799Z",
     "start_time": "2025-10-08T04:49:55.011203Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# CATEGORY 14: INTERACTION FEATURES\n",
    "def engineer_interaction_features(df, interaction_pairs):\n",
    "    \"\"\"\n",
    "    Generate multiplicative interaction features between specified column pairs (Features 175-180)\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame containing the features\n",
    "    interaction_pairs : list of tuples\n",
    "        List of column pairs to create interactions, e.g. [('mn_grade_pct', 'ore_density_sg')]\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame with new interaction features added\n",
    "    \"\"\"\n",
    "    print(\"\\nEngineering Interaction Features...\")\n",
    "\n",
    "    interactions_created = 0\n",
    "\n",
    "    for col1, col2 in interaction_pairs:\n",
    "        if col1 in df.columns and col2 in df.columns:\n",
    "            df[f'{col1}_x_{col2}'] = df[col1] * df[col2]\n",
    "            interactions_created += 1\n",
    "        else:\n",
    "            print(f\"  Warning: Columns {col1} or {col2} not found, skipping\")\n",
    "\n",
    "    print(f\"  Generated {interactions_created} interaction features\")\n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "interaction_pairs = [('mn_grade_pct', 'ore_density_sg'), ('flotation_recovery', 'collector_dosage_gt')]\n",
    "engineered_datasets['engineered_flotation_interactions'] = engineer_interaction_features(\n",
    "     engineered_datasets['engineered_flotation'],\n",
    "     interaction_pairs\n",
    ")\n"
   ],
   "id": "8241857140d6befb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Engineering Interaction Features...\n",
      "  Warning: Columns mn_grade_pct or ore_density_sg not found, skipping\n",
      "  Generated 1 interaction features\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T04:50:19.384051Z",
     "start_time": "2025-10-08T04:49:55.125381Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# SAVING ENGINEERED DATASETS TO ENGINEERED_DATA DIRECTORY INSIDE THE DATA DIRECTORY.\n",
    "# Base project directory\n",
    "base_dir = os.path.dirname(os.getcwd())\n",
    "\n",
    "# New directory path for engineered datasets (this will create a folder named 'engineered_data' inside 'data/')\n",
    "engineered_data = os.path.join(base_dir, \"data\", \"engineered_data\")\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(engineered_data, exist_ok=True)\n",
    "\n",
    "print(f\"Engineered datasets will be saved in: {engineered_data}\")\n",
    "\n",
    "for name, df in engineered_datasets.items():\n",
    "    save_path = os.path.join(engineered_data, f\"{name}.csv\")\n",
    "    df.to_csv(save_path, index=False)\n",
    "    print(f\"Saved {name} to {save_path}\")\n"
   ],
   "id": "5f6eb2fd20c68ca1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineered datasets will be saved in: /home/darlenewendie/PycharmProjects/Intelligent-Manganese-Processing-Plant-Optimization/data/engineered_data\n",
      "Saved engineered_ore_feed to /home/darlenewendie/PycharmProjects/Intelligent-Manganese-Processing-Plant-Optimization/data/engineered_data/engineered_ore_feed.csv\n",
      "Saved engineered_crushing to /home/darlenewendie/PycharmProjects/Intelligent-Manganese-Processing-Plant-Optimization/data/engineered_data/engineered_crushing.csv\n",
      "Saved engineered_separation to /home/darlenewendie/PycharmProjects/Intelligent-Manganese-Processing-Plant-Optimization/data/engineered_data/engineered_separation.csv\n",
      "Saved engineered_flotation to /home/darlenewendie/PycharmProjects/Intelligent-Manganese-Processing-Plant-Optimization/data/engineered_data/engineered_flotation.csv\n",
      "Saved engineered_dms to /home/darlenewendie/PycharmProjects/Intelligent-Manganese-Processing-Plant-Optimization/data/engineered_data/engineered_dms.csv\n",
      "Saved engineered_jigging to /home/darlenewendie/PycharmProjects/Intelligent-Manganese-Processing-Plant-Optimization/data/engineered_data/engineered_jigging.csv\n",
      "Saved engineered_dewatering to /home/darlenewendie/PycharmProjects/Intelligent-Manganese-Processing-Plant-Optimization/data/engineered_data/engineered_dewatering.csv\n",
      "Saved engineered_equipment to /home/darlenewendie/PycharmProjects/Intelligent-Manganese-Processing-Plant-Optimization/data/engineered_data/engineered_equipment.csv\n",
      "Saved engineered_energy to /home/darlenewendie/PycharmProjects/Intelligent-Manganese-Processing-Plant-Optimization/data/engineered_data/engineered_energy.csv\n",
      "Saved engineered_flotation_lagged to /home/darlenewendie/PycharmProjects/Intelligent-Manganese-Processing-Plant-Optimization/data/engineered_data/engineered_flotation_lagged.csv\n",
      "Saved cross_dataset_features_snapshot to /home/darlenewendie/PycharmProjects/Intelligent-Manganese-Processing-Plant-Optimization/data/engineered_data/cross_dataset_features_snapshot.csv\n",
      "Saved cross_dataset_features_monthly to /home/darlenewendie/PycharmProjects/Intelligent-Manganese-Processing-Plant-Optimization/data/engineered_data/cross_dataset_features_monthly.csv\n",
      "Saved cross_dataset_features_yearly to /home/darlenewendie/PycharmProjects/Intelligent-Manganese-Processing-Plant-Optimization/data/engineered_data/cross_dataset_features_yearly.csv\n",
      "Saved engineered_flotation_interactions to /home/darlenewendie/PycharmProjects/Intelligent-Manganese-Processing-Plant-Optimization/data/engineered_data/engineered_flotation_interactions.csv\n"
     ]
    }
   ],
   "execution_count": 42
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "manganese_venv",
   "language": "python",
   "display_name": "Manganese Processing (venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
