{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T17:45:56.412522Z",
     "start_time": "2025-10-15T17:45:56.400700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import (mean_absolute_error, mean_squared_error, r2_score, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, roc_auc_score, roc_curve)\n",
    "import xgboost as xgb\n",
    "\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os"
   ],
   "id": "f4c15d885551a833",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T17:45:59.034440Z",
     "start_time": "2025-10-15T17:45:59.020960Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get the datasets and initialize dictionaries\n",
    "\n",
    "BASE_DIR = os.path.dirname(os.path.dirname(os.getcwd()))# Current working directory of the notebook\n",
    "data_dir = os.path.join(BASE_DIR, 'data', 'engineered_data')\n",
    "\n",
    "print(\"Using data directory:\", data_dir)\n",
    "\n",
    "models = {}\n",
    "scalers = {}\n",
    "results = {}\n",
    "\n"
   ],
   "id": "cdc53a6899d6e956",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data directory: /home/darlenewendie/PycharmProjects/Intelligent-Manganese-Processing-Plant-Optimization/data/engineered_data\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T17:46:02.732302Z",
     "start_time": "2025-10-15T17:46:01.337508Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Loading engineered datasets\n",
    "print(\"LOADING ENGINEERED DATASETS\")\n",
    "datasets = {}\n",
    "\n",
    "files = {\n",
    "    'separation': 'engineered_separation.csv',\n",
    "    'floatation': 'engineered_flotation.csv',\n",
    "    'equipment':'engineered_equipment.csv',\n",
    "    'crushing': 'engineered_crushing.csv',\n",
    "    'dms': 'engineered_dms.csv'\n",
    "}\n",
    "\n",
    "for name, filename in files.items():\n",
    "    filepath  = os.path.join(data_dir, filename)\n",
    "    try:\n",
    "        df= pd.read_csv(filepath, parse_dates=['timestamp'])\n",
    "        datasets[name] = df\n",
    "        print(f\"Loaded {name}:  {len(df): }records and {len(df.columns) :} columns\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File Not found:  {filepath}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error Loading: {name}: {str(e)}\")\n",
    "\n",
    "print(f\"Total datasets loaded: {len(datasets)}\")\n"
   ],
   "id": "9ff111f164b3e207",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADING ENGINEERED DATASETS\n",
      "Loaded separation:   12000records and 32 columns\n",
      "Loaded floatation:   12000records and 46 columns\n",
      "Loaded equipment:   8000records and 27 columns\n",
      "Loaded crushing:   15000records and 26 columns\n",
      "Loaded dms:   8000records and 29 columns\n",
      "Total datasets loaded: 5\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T17:35:02.666119Z",
     "start_time": "2025-10-15T17:35:02.608301Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# MODEL 1: RECOVERY PREDICTION\n",
    "\n",
    "def prepare_recovery_data(datasets):\n",
    "    print(\"MODEL 1: RECOVERY - PREDICTION - DATA PREPARATION\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Use multiple datasets for recovery prediction\n",
    "    recovery_data = []\n",
    "\n",
    "    # Gravity separation in recovery\n",
    "    if 'separation' in datasets:\n",
    "        sep_df = datasets['separation'].copy()\n",
    "\n",
    "        # Select the Features for gravity separation to e used in ore-recovery prediction\n",
    "        gravity_features = [\n",
    "            # Ore characteristics\n",
    "            'feed_grade_pct' , 'ore_quality_index', 'mn_to_fe_ratio',\n",
    "            # Process parameters\n",
    "            'spiral_speed_rpm', 'wash_water_m3h', 'feed_density_pct_solids',\n",
    "            'spiral_speed_deviation', 'water_to_solids',\n",
    "            # Equipment/ performance\n",
    "            'spiral_concentrate_grade_pct', 'upgrade_ratio', 'separation_sharpness',\n",
    "            #Magnetic separation\n",
    "            'magnetic_intensity_t', ' belt_speed_ms', 'magnetic_intensity_effect',\n",
    "            # Engineered datasets\n",
    "            'grade_recovery_product', 'enrichment_index', 'separation_selectivity',\n",
    "            'overall_enrichment', 'combined_efficiency'\n",
    "        ]\n",
    "\n",
    "        # Check which features exist\n",
    "        available_gravity_features = [f for f in gravity_features if f in sep_df.columns]\n",
    "\n",
    "        if 'overall_recovery' in sep_df.columns and len(available_gravity_features) > 5:\n",
    "            gravity_df = sep_df[available_gravity_features + ['overall_recovery']].copy()\n",
    "            gravity_df['circuit_type'] = 'gravity'\n",
    "            recovery_data.append(gravity_df)\n",
    "            print(f\"✓ Gravity separation: {len(gravity_df)} records, {len(available_gravity_features)} features\")\n",
    "\n",
    "    # Flotation recovery\n",
    "    if 'flotation' in datasets:\n",
    "        flot_df = datasets['flotation'].copy()\n",
    "\n",
    "        flotation_features = [\n",
    "            # Feed characteristics\n",
    "            'feed_grade_pct', 'ore_quality_index',\n",
    "            # Reagent parameters\n",
    "            'collector_dosage_gt', 'frother_dosage_gt', 'reagent_efficiency',\n",
    "            'collector_intensity', 'collector_to_frother_ratio',\n",
    "            # pH control\n",
    "            'ph_value', 'ph_deviation_from_optimal', 'ph_in_optimal_range',\n",
    "            # Process parameters\n",
    "            'pulp_density_pct_solids', 'air_flow_m3_min', 'residence_time_min',\n",
    "            'air_to_solids_ratio', 'flotation_kinetics_factor',\n",
    "            # Equipment health (if available)\n",
    "            'cell_health_score', 'blower_health_score',\n",
    "            'cell_health_recovery_product', 'blower_efficiency_factor',\n",
    "            # Performance indicators\n",
    "            'concentrate_grade_pct', 'froth_stability_index', 'froth_loading'\n",
    "        ]\n",
    "\n",
    "        available_flotation_features = [f for f in flotation_features if f in flot_df.columns]\n",
    "\n",
    "        if 'flotation_recovery' in flot_df.columns and len(available_flotation_features) > 5:\n",
    "            flotation_df = flot_df[available_flotation_features + ['flotation_recovery']].copy()\n",
    "            flotation_df.rename(columns={'flotation_recovery': 'overall_recovery'}, inplace=True)\n",
    "            flotation_df['circuit_type'] = 'flotation'\n",
    "            recovery_data.append(flotation_df)\n",
    "            print(f\"✓ Flotation: {len(flotation_df)} records, {len(available_flotation_features)} features\")\n",
    "\n",
    "    # DMS recovery\n",
    "    if 'dms' in datasets:\n",
    "        dms_df = datasets['dms'].copy()\n",
    "\n",
    "        dms_features = [\n",
    "            'feed_grade_pct', 'feed_size_mm', 'media_density_sg', 'cyclone_pressure_kpa',\n",
    "            'density_differential', 'separation_sharpness_dms', 'media_efficiency',\n",
    "            'pressure_utilization', 'media_consumption_efficiency',\n",
    "            'size_suitability_for_dms', 'coarse_fraction_ratio',\n",
    "            'cyclone_health_score', 'cyclone_health_efficiency_product',\n",
    "            'sink_grade_pct', 'dms_upgrade_factor'\n",
    "        ]\n",
    "\n",
    "        available_dms_features = [f for f in dms_features if f in dms_df.columns]\n",
    "\n",
    "        if 'dms_recovery' in dms_df.columns and len(available_dms_features) > 5:\n",
    "            dms_data = dms_df[available_dms_features + ['dms_recovery']].copy()\n",
    "            dms_data.rename(columns={'dms_recovery': 'overall_recovery'}, inplace=True)\n",
    "            dms_data['circuit_type'] = 'dms'\n",
    "            recovery_data.append(dms_data)\n",
    "            print(f\"✓ DMS: {len(dms_data)} records, {len(available_dms_features)} features\")\n",
    "\n",
    "    if not recovery_data:\n",
    "        raise ValueError(\"No recovery data available!\")\n",
    "\n",
    "    # Combine all recovery data\n",
    "    combined_recovery = pd.concat(recovery_data, ignore_index=True)\n",
    "\n",
    "    print(f\"\\n✓ Combined dataset: {combined_recovery.shape}\")\n",
    "    print(f\"  Target variable: overall_recovery\")\n",
    "    print(f\"  Range: {combined_recovery['overall_recovery'].min():.3f} - {combined_recovery['overall_recovery'].max():.3f}\")\n",
    "    print(f\"  Mean: {combined_recovery['overall_recovery'].mean():.3f}\")\n",
    "\n",
    "    return combined_recovery\n",
    "\n",
    "def train_recovery_model(self, recovery_data):\n",
    "    \"\"\"Train recovery prediction model\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"MODEL 1: TRAINING RECOVERY PREDICTION MODEL\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Handle missing values\n",
    "    print(\"\\n1. Handling Missing Values...\")\n",
    "    missing_before = recovery_data.isnull().sum().sum()\n",
    "    print(f\"   Missing values before: {missing_before}\")\n",
    "\n",
    "    # For lag features, forward fill then backward fill\n",
    "    recovery_data = recovery_data.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "    # For remaining NaN, fill with median\n",
    "    for col in recovery_data.select_dtypes(include=[np.number]).columns:\n",
    "        if recovery_data[col].isnull().any():\n",
    "            recovery_data[col].fillna(recovery_data[col].median(), inplace=True)\n",
    "\n",
    "    missing_after = recovery_data.isnull().sum().sum()\n",
    "    print(f\"   Missing values after: {missing_after}\")\n",
    "\n",
    "    # Encode categorical variables\n",
    "    if 'circuit_type' in recovery_data.columns:\n",
    "        le = LabelEncoder()\n",
    "        recovery_data['circuit_type_encoded'] = le.fit_transform(recovery_data['circuit_type'])\n",
    "\n",
    "    # Separate features and target\n",
    "    target = 'overall_recovery'\n",
    "    exclude_cols = [target, 'circuit_type', 'timestamp'] if 'timestamp' in recovery_data.columns else [target, 'circuit_type']\n",
    "\n",
    "    feature_cols = [col for col in recovery_data.columns if col not in exclude_cols]\n",
    "\n",
    "    X = recovery_data[feature_cols]\n",
    "    y = recovery_data[target]\n",
    "\n",
    "    print(f\"\\n2. Feature Selection...\")\n",
    "    print(f\"   Total features: {len(feature_cols)}\")\n",
    "    print(f\"   Sample features: {feature_cols[:5]}\")\n",
    "\n",
    "    # Train-test split (80-20)\n",
    "    print(f\"\\n3. Train-Test Split...\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    print(f\"   Training set: {X_train.shape}\")\n",
    "    print(f\"   Test set: {X_test.shape}\")\n",
    "\n",
    "    # Feature scaling\n",
    "    print(f\"\\n4. Feature Scaling...\")\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    self.scalers['recovery'] = scaler\n",
    "\n",
    "    # Train Random Forest\n",
    "    print(f\"\\n5. Training Random Forest Regressor...\")\n",
    "    rf_model = RandomForestRegressor(\n",
    "        n_estimators=200,\n",
    "        max_depth=15,\n",
    "        min_samples_split=10,\n",
    "        min_samples_leaf=4,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Train XGBoost\n",
    "    print(f\"\\n6. Training XGBoost Regressor...\")\n",
    "    xgb_model = xgb.XGBRegressor(\n",
    "        n_estimators=200,\n",
    "        max_depth=8,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    xgb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Evaluate both models\n",
    "    print(f\"\\n7. Model Evaluation...\")\n",
    "\n",
    "    # Random Forest predictions\n",
    "    y_pred_rf_train = rf_model.predict(X_train_scaled)\n",
    "    y_pred_rf_test = rf_model.predict(X_test_scaled)\n",
    "\n",
    "    # XGBoost predictions\n",
    "    y_pred_xgb_train = xgb_model.predict(X_train_scaled)\n",
    "    y_pred_xgb_test = xgb_model.predict(X_test_scaled)\n",
    "\n",
    "    # Calculate metrics\n",
    "    rf_results = {\n",
    "        'train': {\n",
    "            'MAE': mean_absolute_error(y_train, y_pred_rf_train),\n",
    "            'RMSE': np.sqrt(mean_squared_error(y_train, y_pred_rf_train)),\n",
    "            'R2': r2_score(y_train, y_pred_rf_train),\n",
    "            'MAPE': np.mean(np.abs((y_train - y_pred_rf_train) / y_train)) * 100\n",
    "        },\n",
    "        'test': {\n",
    "            'MAE': mean_absolute_error(y_test, y_pred_rf_test),\n",
    "            'RMSE': np.sqrt(mean_squared_error(y_test, y_pred_rf_test)),\n",
    "            'R2': r2_score(y_test, y_pred_rf_test),\n",
    "            'MAPE': np.mean(np.abs((y_test - y_pred_rf_test) / y_test)) * 100\n",
    "        }\n",
    "    }\n",
    "\n",
    "    xgb_results = {\n",
    "        'train': {\n",
    "            'MAE': mean_absolute_error(y_train, y_pred_xgb_train),\n",
    "            'RMSE': np.sqrt(mean_squared_error(y_train, y_pred_xgb_train)),\n",
    "            'R2': r2_score(y_train, y_pred_xgb_train),\n",
    "            'MAPE': np.mean(np.abs((y_train - y_pred_xgb_train) / y_train)) * 100\n",
    "        },\n",
    "        'test': {\n",
    "            'MAE': mean_absolute_error(y_test, y_pred_xgb_test),\n",
    "            'RMSE': np.sqrt(mean_squared_error(y_test, y_pred_xgb_test)),\n",
    "            'R2': r2_score(y_test, y_pred_xgb_test),\n",
    "            'MAPE': np.mean(np.abs((y_test - y_pred_xgb_test) / y_test)) * 100\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Display results\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RANDOM FOREST RESULTS:\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Training Set:\")\n",
    "    for metric, value in rf_results['train'].items():\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "    print(\"\\nTest Set:\")\n",
    "    for metric, value in rf_results['test'].items():\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"XGBOOST RESULTS:\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Training Set:\")\n",
    "    for metric, value in xgb_results['train'].items():\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "    print(\"\\nTest Set:\")\n",
    "    for metric, value in xgb_results['test'].items():\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "    # Select best model\n",
    "    if rf_results['test']['R2'] > xgb_results['test']['R2']:\n",
    "        best_model = rf_model\n",
    "        best_model_name = 'Random Forest'\n",
    "        best_results = rf_results\n",
    "        y_pred_test = y_pred_rf_test\n",
    "    else:\n",
    "        best_model = xgb_model\n",
    "        best_model_name = 'XGBoost'\n",
    "        best_results = xgb_results\n",
    "        y_pred_test = y_pred_xgb_test\n",
    "\n",
    "    print(f\"\\n✓ Best Model: {best_model_name} (Test R² = {best_results['test']['R2']:.4f})\")\n",
    "\n",
    "    # Feature importance\n",
    "    if hasattr(best_model, 'feature_importances_'):\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': feature_cols,\n",
    "            'importance': best_model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "\n",
    "        print(f\"\\nTop 10 Most Important Features:\")\n",
    "        print(feature_importance.head(10).to_string(index=False))\n",
    "\n",
    "    # Store results\n",
    "    self.models['recovery'] = {\n",
    "        'model': best_model,\n",
    "        'model_name': best_model_name,\n",
    "        'features': feature_cols,\n",
    "        'results': best_results,\n",
    "        'feature_importance': feature_importance if hasattr(best_model, 'feature_importances_') else None\n",
    "    }\n",
    "\n",
    "    # Visualizations\n",
    "    self.plot_recovery_results(y_test, y_pred_test, best_results, best_model_name)\n",
    "\n",
    "    return best_model, best_results\n",
    "\n",
    "def plot_recovery_results(self, y_test, y_pred, results, model_name):\n",
    "    \"\"\"Plot recovery model results\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "    # Actual vs Predicted\n",
    "    axes[0, 0].scatter(y_test, y_pred, alpha=0.5, s=20)\n",
    "    axes[0, 0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()],\n",
    "                   'r--', linewidth=2, label='Perfect Prediction')\n",
    "    axes[0, 0].set_xlabel('Actual Recovery')\n",
    "    axes[0, 0].set_ylabel('Predicted Recovery')\n",
    "    axes[0, 0].set_title(f'{model_name}: Actual vs Predicted\\nR² = {results[\"test\"][\"R2\"]:.4f}')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Residuals\n",
    "    residuals = y_test - y_pred\n",
    "    axes[0, 1].scatter(y_pred, residuals, alpha=0.5, s=20)\n",
    "    axes[0, 1].axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "    axes[0, 1].set_xlabel('Predicted Recovery')\n",
    "    axes[0, 1].set_ylabel('Residuals')\n",
    "    axes[0, 1].set_title('Residual Plot')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # Residual distribution\n",
    "    axes[1, 0].hist(residuals, bins=50, edgecolor='black', alpha=0.7)\n",
    "    axes[1, 0].axvline(x=0, color='r', linestyle='--', linewidth=2)\n",
    "    axes[1, 0].set_xlabel('Residual Value')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "    axes[1, 0].set_title(f'Residual Distribution\\nMAE = {results[\"test\"][\"MAE\"]:.4f}')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Error distribution\n",
    "    errors = np.abs(residuals)\n",
    "    axes[1, 1].hist(errors, bins=50, edgecolor='black', alpha=0.7, color='orange')\n",
    "    axes[1, 1].set_xlabel('Absolute Error')\n",
    "    axes[1, 1].set_ylabel('Frequency')\n",
    "    axes[1, 1].set_title(f'Error Distribution\\nRMSE = {results[\"test\"][\"RMSE\"]:.4f}')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('./model1_recovery_results.png', dpi=300, bbox_inches='tight')\n",
    "    print(f\"\\n✓ Plots saved to: ./model1_recovery_results.png\")\n",
    "    plt.show()\n"
   ],
   "id": "c53669f86602a368",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# MODEL 2: EQUIPMENT FAILURE PREDICTION - DATA PREPARATION\n",
    "\n",
    "def prepare_equipment_failure_data(datasets):\n",
    "        \"\"\"Prepare data for equipment failure prediction\"\"\"\n",
    "        print(\"MODEL 2: EQUIPMENT FAILURE PREDICTION - DATA PREPARATION\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        if 'equipment' not in datasets:\n",
    "            raise ValueError(\"Equipment dataset not found!\")\n",
    "\n",
    "        equipment_df = datasets['equipment'].copy()\n",
    "\n",
    "        # Create failure target variable (failure within 30 days)\n",
    "        # Using failure_probability as proxy: >0.3 = high risk of failure\n",
    "        equipment_df['failure_within_30_days'] = (equipment_df['failure_probability'] > 0.3).astype(int)\n",
    "\n",
    "        print(f\"✓ Created target variable: failure_within_30_days\")\n",
    "        print(f\"  Failure cases: {equipment_df['failure_within_30_days'].sum()} ({equipment_df['failure_within_30_days'].mean()*100:.1f}%)\")\n",
    "        print(f\"  Normal cases: {(equipment_df['failure_within_30_days']==0).sum()} ({(1-equipment_df['failure_within_30_days'].mean())*100:.1f}%)\")\n",
    "\n",
    "        # Select features for failure prediction\n",
    "        failure_features = [\n",
    "            # Core health metrics\n",
    "            'health_score', 'operating_hours', 'rul_days',\n",
    "            # Condition monitoring\n",
    "            'vibration_rms', 'temperature_c', 'power_factor',\n",
    "            # Engineered features\n",
    "            'health_degradation_rate', 'equipment_age_factor',\n",
    "            'vibration_severity_index', 'vibration_health_ratio',\n",
    "            'temperature_deviation', 'thermal_stress_index',\n",
    "            'power_factor_degradation', 'efficiency_loss_estimate',\n",
    "            # Equipment specific\n",
    "            'wear_rate_pct', 'health_wear_product', 'maintenance_urgency_score'\n",
    "        ]\n",
    "\n",
    "        # Lag features if available\n",
    "        lag_features = [col for col in equipment_df.columns if 'lag' in col or 'rolling' in col]\n",
    "        failure_features.extend(lag_features[:10])  # Add first 10 lag features\n",
    "\n",
    "        # Check which features exist\n",
    "        available_features = [f for f in failure_features if f in equipment_df.columns]\n",
    "\n",
    "        print(f\"\\n✓ Selected {len(available_features)} features for failure prediction\")\n",
    "        print(f\"  Sample features: {available_features[:5]}\")\n",
    "\n",
    "        # Create final dataset\n",
    "        failure_data = equipment_df[available_features + ['failure_within_30_days', 'equipment_type']].copy()\n",
    "\n",
    "        print(f\"\\n✓ Dataset shape: {failure_data.shape}\")\n",
    "\n",
    "        return failure_data\n",
    "\n",
    "    def train_failure_prediction_model(self, failure_data):\n",
    "        \"\"\"Train equipment failure prediction model\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"MODEL 2: TRAINING FAILURE PREDICTION MODEL\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        # Handle missing values\n",
    "        print(\"\\n1. Handling Missing Values...\")\n",
    "        missing_before = failure_data.isnull().sum().sum()\n",
    "        print(f\"   Missing values before: {missing_before}\")\n",
    "\n",
    "        failure_data = failure_data.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "        for col in failure_data.select_dtypes(include=[np.number]).columns:\n",
    "            if failure_data[col].isnull().any():\n",
    "                failure_data[col].fillna(failure_data[col].median(), inplace=True)\n",
    "\n",
    "        missing_after = failure_data.isnull().sum().sum()\n",
    "        print(f\"   Missing values after: {missing_after}\")\n",
    "\n",
    "        # Encode equipment type\n",
    "        if 'equipment_type' in failure_data.columns:\n",
    "            le = LabelEncoder()\n",
    "            failure_data['equipment_type_encoded'] = le.fit_transform(failure_data['equipment_type'])\n",
    "\n",
    "        # Separate features and target\n",
    "        target = 'failure_within_30_days'\n",
    "        exclude_cols = [target, 'equipment_type', 'timestamp'] if 'timestamp' in failure_data.columns else [target, 'equipment_type']\n",
    "\n",
    "        feature_cols = [col for col in failure_data.columns if col not in exclude_cols]\n",
    "\n",
    "        X = failure_data[feature_cols]\n",
    "        y = failure_data[target]\n",
    "\n",
    "        print(f\"\\n2. Feature Selection...\")\n",
    "        print(f\"   Total features: {len(feature_cols)}\")\n",
    "\n",
    "        # Train-test split (stratified)\n",
    "        print(f\"\\n3. Train-Test Split (Stratified)...\")\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "        print(f\"   Training set: {X_train.shape}\")\n",
    "        print(f\"   Test set: {X_test.shape}\")\n",
    "        print(f\"   Training failures: {y_train.sum()} ({y_train.mean()*100:.1f}%)\")\n",
    "        print(f\"   Test failures: {y_test.sum()} ({y_test.mean()*100:.1f}%)\")\n",
    "\n",
    "        # Feature scaling\n",
    "        print(f\"\\n4. Feature Scaling...\")\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        self.scalers['failure'] = scaler\n",
    "\n",
    "        # Train Random Forest Classifier\n",
    "        print(f\"\\n5. Training Random Forest Classifier...\")\n",
    "        rf_clf = RandomForestClassifier(\n",
    "            n_estimators=200,\n",
    "            max_depth=15,\n",
    "            min_samples_split=10,\n",
    "            min_samples_leaf=4,\n",
    "            class_weight='balanced',  # Handle imbalanced data\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "        rf_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "        # Train XGBoost Classifier\n",
    "        print(f\"\\n6. Training XGBoost Classifier...\")\n",
    "\n",
    "        # Calculate scale_pos_weight for imbalanced data\n",
    "        scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "\n",
    "        xgb_clf = xgb.XGBClassifier(\n",
    "            n_estimators=200,\n",
    "            max_depth=8,\n",
    "            learning_rate=0.05,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            scale_pos_weight=scale_pos_weight,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        xgb_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "        # Evaluate both models\n",
    "        print(f\"\\n7. Model Evaluation...\")\n",
    "\n",
    "        # Random Forest predictions\n",
    "        y_pred_rf_train = rf_clf.predict(X_train_scaled)\n",
    "        y_pred_rf_test = rf_clf.predict(X_test_scaled)\n",
    "        y_pred_rf_proba = rf_clf.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "        # XGBoost predictions\n",
    "        y_pred_xgb_train = xgb_clf.predict(X_train_scaled)\n",
    "        y_pred_xgb_test = xgb_clf.predict(X_test_scaled)\n",
    "        y_pred_xgb_proba = xgb_clf.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "        # Calculate metrics\n",
    "        rf_results = {\n",
    "            'train': {\n",
    "                'Accuracy': accuracy_score(y_train, y_pred_rf_train),\n",
    "                'Precision': precision_score(y_train, y_pred_rf_train, zero_division=0),\n",
    "                'Recall': recall_score(y_train, y_pred_rf_train, zero_division=0),\n",
    "                'F1': f1_score(y_train, y_pred_rf_train, zero_division=0)\n",
    "            },\n",
    "            'test': {\n",
    "                'Accuracy': accuracy_score(y_test, y_pred_rf_test),\n",
    "                'Precision': precision_score(y_test, y_pred_rf_test, zero_division=0),\n",
    "                'Recall': recall_score(y_test, y_pred_rf_test, zero_division=0),\n",
    "                'F1': f1_score(y_test, y_pred_rf_test, zero_division=0),\n",
    "                'ROC-AUC': roc_auc_score(y_test, y_pred_rf_proba)\n",
    "            },\n",
    "            'confusion_matrix': confusion_matrix(y_test, y_pred_rf_test),\n",
    "            'y_pred_proba': y_pred_rf_proba\n",
    "        }\n",
    "\n",
    "        xgb_results = {\n",
    "            'train': {\n",
    "                'Accuracy': accuracy_score(y_train, y_pred_xgb_train),\n",
    "                'Precision': precision_score(y_train, y_pred_xgb_train, zero_division=0),\n",
    "                'Recall': recall_score(y_train, y_pred_xgb_train, zero_division=0),\n",
    "                'F1': f1_score(y_train, y_pred_xgb_train, zero_division=0)\n",
    "            },\n",
    "            'test': {\n",
    "                'Accuracy': accuracy_score(y_test, y_pred_xgb_test),\n",
    "                'Precision': precision_score(y_test, y_pred_xgb_test, zero_division=0),\n",
    "                'Recall': recall_score(y_test, y_pred_xgb_test, zero_division=0),\n",
    "                'F1': f1_score(y_test, y_pred_xgb_test, zero_division=0),\n",
    "                'ROC-AUC': roc_auc_score(y_test, y_pred_xgb_proba)\n",
    "            },\n",
    "            'confusion_matrix': confusion_matrix(y_test, y_pred_xgb_test),\n",
    "            'y_pred_proba': y_pred_xgb_proba\n",
    "        }\n",
    "\n",
    "        # Display results\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"RANDOM FOREST CLASSIFIER RESULTS:\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"Training Set:\")\n",
    "        for metric, value in rf_results['train'].items():\n",
    "            print(f\"  {metric}: {value:.4f}\")\n",
    "        print(\"\\nTest Set:\")\n",
    "        for metric, value in rf_results['test'].items():\n",
    "            print(f\"  {metric}: {value:.4f}\")\n",
    "        print(\"\\nConfusion Matrix (Test):\")\n",
    "        print(rf_results['confusion_matrix'])\n",
    "\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"XGBOOST CLASSIFIER RESULTS:\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"Training Set:\")\n",
    "        for metric, value in xgb_results['train'].items():\n",
    "            print(f\"  {metric}: {value:.4f}\")\n",
    "        print(\"\\nTest Set:\")\n",
    "        for metric, value in xgb_results['test'].items():\n",
    "            print(f\"  {metric}: {value:.4f}\")\n",
    "        print(\"\\nConfusion Matrix (Test):\")\n",
    "        print(xgb_results['confusion_matrix'])\n",
    "\n",
    "        # Select best model (prioritize recall for safety)\n",
    "        if rf_results['test']['Recall'] > xgb_results['test']['Recall']:\n",
    "            best_model = rf_clf\n",
    "            best_model_name = 'Random Forest'\n",
    "            best_results = rf_results\n",
    "            y_pred_test = y_pred_rf_test\n",
    "            y_pred_proba = y_pred_rf_proba\n",
    "        else:\n",
    "            best_model = xgb_clf\n",
    "            best_model_name = 'XGBoost'\n",
    "            best_results = xgb_results\n",
    "            y_pred_test = y_pred_xgb_test\n",
    "            y_pred_proba = y_pred_xgb_proba\n",
    "\n",
    "        print(f\"\\n✓ Best Model: {best_model_name} (Recall = {best_results['test']['Recall']:.4f})\")\n",
    "        print(f\"  Note: Recall prioritized to minimize missed failures (safety critical)\")\n",
    "\n",
    "        # Feature importance\n",
    "        if hasattr(best_model, 'feature_importances_'):\n",
    "            feature_importance = pd.DataFrame({\n",
    "                'feature': feature_cols,\n",
    "                'importance': best_model.feature_importances_\n",
    "            }).sort_values('importance', ascending=False)\n",
    "\n",
    "            print(f\"\\nTop 10 Most Important Features for Failure Prediction:\")\n",
    "            print(feature_importance.head(10).to_string(index=False))\n",
    "\n",
    "        # Store results\n",
    "        self.models['failure'] = {\n",
    "            'model': best_model,\n",
    "            'model_name': best_model_name,\n",
    "            'features': feature_cols,\n",
    "            'results': best_results,\n",
    "            'feature_importance': feature_importance if hasattr(best_model, 'feature_importances_') else None\n",
    "        }\n",
    "\n",
    "        # Visualizations\n",
    "        self.plot_failure_results(y_test, y_pred_test, y_pred_proba, best_results, best_model_name)\n",
    "\n",
    "        return best_model, best_results\n",
    "\n",
    "    def plot_failure_results(self, y_test, y_pred, y_pred_proba, results, model_name):\n",
    "        \"\"\"Plot failure prediction model results\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "        # Confusion Matrix\n",
    "        cm = results['confusion_matrix']\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0, 0],\n",
    "                   xticklabels=['No Failure', 'Failure'],\n",
    "                   yticklabels=['No Failure', 'Failure'])\n",
    "        axes[0, 0].set_xlabel('Predicted')\n",
    "        axes[0, 0].set_ylabel('Actual')\n",
    "        axes[0, 0].set_title(f'{model_name}: Confusion Matrix\\nAccuracy = {results[\"test\"][\"Accuracy\"]:.4f}')\n",
    "\n",
    "        # ROC Curve\n",
    "        fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "        axes[0, 1].plot(fpr, tpr, linewidth=2, label=f'ROC (AUC = {results[\"test\"][\"ROC-AUC\"]:.4f})')\n",
    "        axes[0, 1].plot([0, 1], [0, 1], 'k--', linewidth=2, label='Random Classifier')\n",
    "        axes[0, 1].set_xlabel('False Positive Rate')\n",
    "        axes[0, 1].set_ylabel('True Positive Rate (Recall)')\n",
    "        axes[0, 1].set_title('ROC Curve')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "        # Precision-Recall Trade-off\n",
    "        precision, recall, pr_thresholds = precision_recall_curve(y_test, y_pred_proba)\n",
    "        axes[1, 0].plot(recall, precision, linewidth=2, color='green')\n",
    "        axes[1, 0].set_xlabel('Recall')\n",
    "        axes[1, 0].set_ylabel('Precision')\n",
    "        axes[1, 0].set_title(f'Precision-Recall Curve\\nF1 = {results[\"test\"][\"F1\"]:.4f}')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "        # Prediction Probability Distribution\n",
    "        axes[1, 1].hist(y_pred_proba[y_test == 0], bins=30, alpha=0.6, label='No Failure', color='blue')\n",
    "        axes[1, 1].hist(y_pred_proba[y_test == 1], bins=30, alpha=0.6, label='Failure', color='red')\n",
    "        axes[1, 1].axvline(x=0.5, color='black', linestyle='--', linewidth=2, label='Threshold (0.5)')\n",
    "        axes[1, 1].set_xlabel('Predicted Probability')\n",
    "        axes[1, 1].set_ylabel('Frequency')\n",
    "        axes[1, 1].set_title('Prediction Probability Distribution')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('./model2_failure_results.png', dpi=300, bbox_inches='tight')\n",
    "        print(f\"\\n✓ Plots saved to: ./model2_failure_results.png\")\n",
    "        plt.show()"
   ],
   "id": "aa864f34845b72b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "manganese_venv",
   "language": "python",
   "display_name": "Manganese Processing (venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
